<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Yeqown</title>
    <link>https://www.yeqown.xyz/</link>
    <description>Recent content on Yeqown</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 27 Sep 2020 11:32:15 +0800</lastBuildDate><atom:link href="https://www.yeqown.xyz/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>WebSocket Implemention With Go</title>
      <link>https://www.yeqown.xyz/2020/09/27/WebSocket-Implemention-With-Go/</link>
      <pubDate>Sun, 27 Sep 2020 11:32:15 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2020/09/27/WebSocket-Implemention-With-Go/</guid>
      <description>什么是WS协议  The WebSocket Protocol enables two-way communication between a client running untrusted code in a controlled environment to a remote host that has opted-in to communications from that code. The security model used for this is the origin-based security model commonly used by web browsers. The protocol consists of an opening handshake followed by basic message framing, layered over TCP.
  The goal of this technology is to provide a mechanism for browser-based applications that need two-way communication with servers that does not rely on opening multiple HTTP connections (e.</description>
    </item>
    
    <item>
      <title>Kubernetes中gRPC Load Balancing分析和解决</title>
      <link>https://www.yeqown.xyz/2020/09/22/Kubernetes%E4%B8%ADgRPC-Load-Balancing%E5%88%86%E6%9E%90%E5%92%8C%E8%A7%A3%E5%86%B3/</link>
      <pubDate>Tue, 22 Sep 2020 13:33:20 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2020/09/22/Kubernetes%E4%B8%ADgRPC-Load-Balancing%E5%88%86%E6%9E%90%E5%92%8C%E8%A7%A3%E5%86%B3/</guid>
      <description>背景 第一次，线上遇到大量接口RT超过10s触发了系统告警，运维反馈k8s集群无异常，负载无明显上升。将报警接口相关的服务重启一番后发现并无改善。但是开发人员使用链路追踪系统发现，比较慢的请求总是某个gRPC服务中的几个POD导致，由其他POD处理的请求并不会出现超时告警。
第二次，同样遇到接口RT超过阈值触发告警，从k8s中查到某个gRPC服务（关键服务）重启次数异常，查看重启原因时发现是OOM Killed，OOM killed并不是负载不均衡直接导致的，但是也有一定的关系，这个后面再说。前两次由于监控不够完善（于我而言，运维的很多面板都没有权限，没办法排查）。期间利用pprof分析了该服务内存泄漏点，并修复上线观察。经过第二次问题并解决之后，线上超时告警恢复正常水平，但是该 deployment 下的几个POD占用资源（Mem / CPU / Network-IO），差距甚大。
 第二张图是运维第一次发现该服务OOM killed 之后调整了内存上限从 512MB =&amp;gt; 1G，然而只是让它死得慢一点而已。 从上面两张图能够石锤的是该服务一定存在内存泄漏。Go项目内存占用的分析，我总结了如下的排查步骤：
 1. 代码泄漏（pprof）（可能原因 goroutine泄漏；闭包） 2. Go Runtime + Linux 内核（RSS虚高导致OOM）https://github.com/golang/go/issues/23687 3. 采集指标不正常（container_memory_working_set_bytes） 2，3 是基于第1点能基本排除代码问题的后续步骤。 解决和排查手段： 1. pprof 通过heap + goroutine 是否异常，来定位泄漏点 运行`go tool pprof`命令时加上--nodefration=0.05参数，表示如果调用的子函数使用的CPU、memory不超过 5%，就忽略它。 2. 确认go版本和内核版本，确认是否开启了MADV_FREE，导致RSS下降不及时(1.12+ 和 linux内核版本大于 4.5)。 3. RSS + Cache 内存检查 &amp;gt; Cache 过大的原因 https://www.cnblogs.com/zh94/p/11922714.html // IO密集：手动释放或者定期重启 查看服务器内存使用情况： `free -g` 查看进程内存情况： `pidstat -rI -p 13744` 查看进程打开的文件： `lsof -p 13744` 查看容器内的PID： `docker inspect --format &amp;#34;{{ .</description>
    </item>
    
    <item>
      <title>近期使用Docker打包镜像遇到的问题总结</title>
      <link>https://www.yeqown.xyz/2020/08/12/%E8%BF%91%E6%9C%9F%E4%BD%BF%E7%94%A8Docker%E6%89%93%E5%8C%85%E9%95%9C%E5%83%8F%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/</link>
      <pubDate>Wed, 12 Aug 2020 09:38:22 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2020/08/12/%E8%BF%91%E6%9C%9F%E4%BD%BF%E7%94%A8Docker%E6%89%93%E5%8C%85%E9%95%9C%E5%83%8F%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/</guid>
      <description>背景 在 github.com/yeqown/goreportcard 项目中我改造了 goreportcard。 后续为了方便部署，我准备将其打包成为docker镜像并上传到 DockerHub。期间遇到了下面的问题，并一一解决，这里做一个记录帮助以后遇到类似的问题可以快速解决。
初期的目标是：将goreportcard和golangci-lint编译好，尽可能较小镜像的体积。因此第一次尝试，我使用了分阶段编译，用golang:1.14.1编译，alpine来发布。
基本 Dockerfile 如下：
# building stageFROMgolang:1.14-alpine3.11 as buildWORKDIR/tmp/buildCOPY . .RUN export GOPROXY=&amp;#34;https://goproxy.cn,direct&amp;#34; \  &amp;amp;&amp;amp; go mod download \  &amp;amp;&amp;amp; go build -o app ./cmd/goreportcard-cli/ \  &amp;amp;&amp;amp; go get github.com/golangci/golangci-lint &amp;amp;&amp;amp; go install github.com/golangci/golangci-lint/cmd/golangci-lint# release stageFROMgolang:1.14-alpine3.11 as releaseWORKDIR/app/goreportcardCOPY --from=build /tmp/build/app .COPY --from=build /tmp/build/tpl ./tplCOPY --from=build /tmp/build/assets ./assets# FIXED: 不能使用golangci-lint, `File not found` 错误COPY --from=build /go/bin/golangci-lint /usr/local/binEXPOSE8000ENTRYPOINT [&amp;#34;./app&amp;#34;, &amp;#34;start-web&amp;#34;, &amp;#34;&amp;amp;&amp;#34;]问题清单和解决方案  由于并不是所有的问题都和Docker有关，因此我会使用 [分类] 在标题上注明。</description>
    </item>
    
    <item>
      <title>Opentracing实战</title>
      <link>https://www.yeqown.xyz/2020/08/06/opentracing%E5%AE%9E%E6%88%98/</link>
      <pubDate>Thu, 06 Aug 2020 08:34:45 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2020/08/06/opentracing%E5%AE%9E%E6%88%98/</guid>
      <description>背景 在没有链路追踪系统的情况下，如果只要少数几个服务，或许还可以通过日志来排查定位问题。但是如果服务一旦超过10个，那么再想通过日志来定位分析问题将无比繁琐。 因为，你先要从大量的日志中删筛选出某次请求的日志数据，才能进行后续的定位分析。 倘若日志系统也不够完善，日志对于调试毫无帮助，那又得退回到最原始的方式，通过代码断点和增加日志，等待问题复现，或者通过肉眼检查代码。 不是说这种方式不行，而是大部分的程序员的业务需求比较紧张，这样的排查手段效率和收益远远达不到要求（如果你有时间，当我没说 🐶）。
在实际场景中，我也遇到了这样的问题：
 日志系统里包含了过少的信息，对于调试几乎没有帮助 (几乎只有错误日志，缺少输出上下文的日志)。 服务调用复杂，一个请求失败，只能透过错误码和错误信息进行判断是否存在调用失败的情况。 调用链路复杂的情况下，想要对某个请求进行优化，无从下手。  这里只列举了跟trace相关的一些原始场景，当然从上面的描述中还能发现日志系统不够完善，对调试不友好，不过这里首要解决的问题是链路追踪问题。
 如果对路链路追踪没有概念，还望自行查阅资料，这里不会过多介绍～
 Opentracing  注意：Opentracing 是一套标准接口，而不是具体实现。
 这里就实战opentracing + jaeger 的链路追踪方案。其中 opentracing 是一套标准接口，而jaeger是包含了 opentracing 的实现的一套工具。 Trace链路简单示例如下：
Trace 描述在分布式系统中的一次&amp;quot;事务&amp;quot;。
Span 表示工作流的一部分的命名和定时操作。可以接受标签(Tag Key:Value)，以及附加到特定span实例的标注(Annotation)，如时间戳和结构化日志。
SpanContext 追踪伴随分布式事务的信息，包括它通过网络或通过消息总线将服务传递给服务的时间。span上下文包含TraceId、SpanId和追踪系统需要传播到下游服务的其他数据。
实战 这里我准备的是 Go 项目，服务之间通过gRPC通信。链路如下：
+-- process internal trace2 | +---&amp;gt; process internal trace1 | | +---&amp;gt; server-b trace(gRPC) entry(HTTP) ---&amp;gt; server-a trace--gRPC--| +---&amp;gt; server-c trace(gRPC) | +----&amp;gt; process internal trace3 从上图中可以明确，我们的目标是：实践跨服务调用和服务内部调用的链路追踪，配合jaeger我们还可以将链路信息可视化。
我有了服务，怎么实施落地？ 为了回答这个问题，我把这个问题结合opentracing的概念再分解一下：</description>
    </item>
    
    <item>
      <title>Channel in Go小结</title>
      <link>https://www.yeqown.xyz/2020/04/13/channel-in-Go%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Mon, 13 Apr 2020 17:23:45 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2020/04/13/channel-in-Go%E5%B0%8F%E7%BB%93/</guid>
      <description>在其他编程语言中，如果想要在线程中通信，最常用的手段是共享内存。然而考虑到线程冲突问题，不得不考虑加锁，以保证并发安全，加锁也一定会带来额外的开销，对性能产生影响。
CSP模型  在 Go 语言中也能使用共享内存加互斥锁进行通信，但是 Go 语言提供了一种不同的并发模型，也就是通信顺序进程（Communicating sequential processes，CSP）1。Goroutine 和 Channel 分别对应 CSP 中的实体和传递信息的媒介，Go 语言中的 Goroutine 会通过 Channel 传递数据。
 使用示例 在使用之前，需要对channel有个整体的印象：
 FIFO (First In First Out) 分为有缓冲和无缓冲两种 在使用过程中会阻塞（无缓冲时，只操作读或写；有缓冲已满时，只操作读或者写） 接受者和发送者都是goroutine  参考下图：
func main() { // Q: 有缓冲和无缓冲在使用上有什么区别？  // ch := make(chan int) // 无缓冲  ch := make(chan int, 1) // 有缓冲，大小为1  // 发送  ch &amp;lt;- 1 fmt.Println(&amp;lt;-ch) } 注意事项 在使用channel时，需要注意一下事项：
   操作\CH状态 ch为空 ch已关闭 ch正常     发送 ch &amp;lt;- 死锁 panic 成功或阻塞   接收 &amp;lt;-ch 死锁 成功或空值 成功或阻塞   关闭 close(ch) panic panic 成功    Q: 这里考虑下如何优雅的关闭channel (避免panic)?</description>
    </item>
    
    <item>
      <title>消息推送架构-Based-GOIM</title>
      <link>https://www.yeqown.xyz/2020/04/02/%E6%B6%88%E6%81%AF%E6%8E%A8%E9%80%81%E6%9E%B6%E6%9E%84-based-GOIM/</link>
      <pubDate>Thu, 02 Apr 2020 14:50:04 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2020/04/02/%E6%B6%88%E6%81%AF%E6%8E%A8%E9%80%81%E6%9E%B6%E6%9E%84-based-GOIM/</guid>
      <description>本文的重点，主要梳理了GOIM的架构，消息流转和消息处理。本文没有提到Comet的具体逻辑，套接字编程和RingBuffer等，但是Comet的复杂度远高于其他两个网元，因此强烈建议阅读Comet源码，应该会对Go网络编程有更多认识。
 GOIM 是Go实现的消息推送的分布式服务，易于扩容伸缩，使用了bilibili/discovery来支持服务发现。相较于我之前用Socket.IO做的信令服务，优点在于更优雅的扩容，将连接层和逻辑层分离，职责更清晰。当然缺点也有（没有和具体实现解耦，如MQ的选型，导致不够灵活；客户端非全双工通信，TCP利用率偏低，这点并不全是缺点，好处是：消息流转清晰，职责非常明确），这部分可以自己做定制（最后的参考文献2中讲很多）。
架构图 总的来说，整个应用的架构如下
 这里省略了其中用于服务发现的“bilibili/discovery”。在整个GOIM中用到服务发现的主要是gRPC和消息推送关系查找。
 如上图：
 Comet负责建立和维持客户端的长连接； Job负责消息的分发； Logic提供三种纬度的消息（全局，ROOM，用户）投递，还包括业务逻辑，Session管理。  消息流转 从上述的架构图中可以知道，消息是通过HTTP调用Logic产生的，然后用MQ来中转（存储，削峰）；每个Job成员都从给队列中消费消息，投递给一个或者多个Comet，由Comet将消息发送给客户端。
生成消息 目前在Github上的GOIM版本，消息（除鉴权/心跳等基础数据包外）生成都是由Logic完成第一手处理，Logic提供了HTTP接口以支持消息发送能力，主要有三个纬度：用户，房间，全应用广播，如下：
curl -d &amp;#39;mid message&amp;#39; http://api.goim.io:3111/goim/push/mids?operation=1000&amp;amp;mids=123 curl -d &amp;#39;room message&amp;#39; http://api.goim.io:3111/goim/push/room?operation=1000&amp;amp;type=live&amp;amp;room=1000 curl -d &amp;#39;broadcast message&amp;#39; http://api.goim.io:3111/goim/push/all?operation=1000 在Logic服务中会通过处理，将消息处理成**附#消息格式#任务队列消息**的格式，然后投递到MQ中。其中三种纬度的消息处理稍有不同：
用户
// goim/internal/logic/push.go // mid =&amp;gt; []PushMsg{op, server, keys, msg} func (l *Logic) PushMids(c context.Context, op int32, mids []int64, msg []byte) (err error) { // 根据用户ID获取所有的 key:server 对应关系；在redis中是一个hash 	keyServers, _, err := l.dao.KeysByMids(c, mids) // .</description>
    </item>
    
    <item>
      <title>Redis主从复制</title>
      <link>https://www.yeqown.xyz/2020/03/29/redis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/</link>
      <pubDate>Sun, 29 Mar 2020 09:45:19 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2020/03/29/redis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/</guid>
      <description>redis主从复制是高可用方案中的一部分，那主从复制是如何进行的？又是如何实现的？怎么支撑了redis的高可用性？在主从模式下Master和Slave节点分别做了哪些事情？
redis高可用方案是什么？ 我理解的redis高可用的特点有：
 高QPS，主从 =&amp;gt; 读写分离 高容量，集群分片 =&amp;gt; 高容量 故障转移，sentinel =&amp;gt; 故障转移 故障恢复，数据持久 =&amp;gt; 故障恢复 ～ 这里我简单的理解（RDB + AOF）= 故障恢复  主从复制  redis 主从复制有两个版本：旧版（Ver2.8-），新版（Ver2.8+，增加PSYNC命令来解决旧版中的问题）
 讨论复制时都需要考虑两种场景：
 场景1：从节点刚刚上线，需要去同步主节点时，这部分可以理解为 全量复制。 场景2：从节点掉线，恢复上线后需要同步数据，使自己和主节点达到一致状态。这部分在旧版复制里等价于全量复制，在新版里可以理解为增量复制。  当然你肯定会想到如果主节点掉线，这时候会怎么样？这个场景当然也在redis高可用方案中，之时不是本文的重点，属于Sentinel机制的内容了。
旧版主从复制 前文说过了，旧版主从复制只有全量复制用于应付上述两个场景，因此下面的流程也只有一份：
 从服务器向主服务器发送sync命令。 主服务器在收到sync命令之后，调用bgsave命令生成最新的rdb文件，将这个文件同步给从服务器，这样从服务器载入这个rdb文件之后，状态就会和主服务器执行bgsave命令时候的一致。 主服务器将保存在命令缓冲区中的写命令同步给从服务器，从服务器执行这些命令，这样从服务器的状态就跟主服务器当前状态一致了。   如果你不知道redis中还有个缓冲区的话，建议系统的了解下redis中缓冲区的设计。这里缓冲区特指命令缓冲区，后面还会讲到复制缓冲区。
 但是这样的实现在 场景2 下的缺点很明显：如果说从节点断线后迅速上线，这段时间内的产生的写命令很少，却要全量复制主库的数据，传输了大量重复数据。
SYNC命令产生的消耗： 1. 主节点生成RDB，需要消耗大量的CPU，内存和磁盘IO 2. 网络传输大量字节数据，需要消耗主从服务器的网络资源 3. 从节点需要从RDB文件恢复，会造成阻塞无法接受客户端请求  优点就是：简单暴力。个人看来在redis架构中不合适的用法，不代表说实际场景中也一定不合适，简单暴力也是一个很大的优点。
新版主从复制 新版的主从复制跟旧版的区别就在于：对场景2的优化。
场景2的缺点上文已经提到过了，那么优化的方向就是**“尽量不使用全量复制；增加增量复制(PSYNC)的功能”**。为此还要解决下列问题：
 如果某个从节点断线了，重新上线该从节点如何知道自己是否应该全量还是增量复制呢？ 该从节点断线恢复后，又怎么知道自己缺失了哪些数据呢？ 主节点又如何补偿该从节点在断线期间丢失的那部分数据呢？旧版的复制除了RDB，还有从命令缓冲区中的写命令来保持数据一致。  为此新版中使用了以下概念：
运行ID - runid 每个redis服务器都有其runid，runid由服务器在启动时自动生成，主服务器会将自己的runid发送给从服务器，而从服务器会将主服务器的runid保存起来。从服务器redis断线重连之后进行同步时，就是根据runid来判断同步的进度：
 如果前后两次主服务器runid一致，则认为这一次断线重连还是之前复制的主服务器，主服务器可以继续尝试部分同步操作。 如果前后两次主服务器runid不相同，则全同步流程。  复制偏移量 - offset 主从节点，分别会维护一个复制偏移量： 主服务器每次向从服务器同步了N字节数据之后，将修改自己的复制偏移量+N。从服务器每次从主服务器同步了N字节数据之后，将修改自己的复制偏移量+N。通过对比主从节点的偏移量很容易就可以发现，主从节点是否处于一致状态。</description>
    </item>
    
    <item>
      <title>Gin源码简要分析</title>
      <link>https://www.yeqown.xyz/2020/01/21/gin%E6%BA%90%E7%A0%81%E7%AE%80%E8%A6%81%E5%88%86%E6%9E%90/</link>
      <pubDate>Tue, 21 Jan 2020 10:05:57 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2020/01/21/gin%E6%BA%90%E7%A0%81%E7%AE%80%E8%A6%81%E5%88%86%E6%9E%90/</guid>
      <description>概述 通过日常对gin场景出发，深入源码，总结介绍gin的核心设计。包含：Engine / HandlerFunc / RouterGroup(Router) / Context。在日常使用中常见的就以上概念，汇总如下：
   概念 解释 应用意义     Engine 引擎 web server的基础支持，也是服务的入口 和 根级数据结构   RouterGroup(Router) 路由 用于支持gin,REST路由绑定和路由匹配的基础，源于radix-tree数据结构支持   HandlerFunc 处理函数 逻辑处理器和中间件实现的函数签名   Context 上下文 封装了请求和响应的操作，为HandlerFunc的定义和中间件模式提供支持    从DEMO开始 type barForm struct { Foo string `form:&amp;#34;foo&amp;#34; binding:&amp;#34;required&amp;#34;` Bar int `form:&amp;#34;bar&amp;#34; binding:&amp;#34;required&amp;#34;` } func (fooHdl FooHdl) Bar(c *gin.Context) { var bform = new(barForm) if err := c.ShouldBind(bform); err !</description>
    </item>
    
    <item>
      <title>一次gRPC使用不当导致goroutine泄漏排查记录</title>
      <link>https://www.yeqown.xyz/2020/01/17/%E8%AE%B0%E4%B8%80%E6%AC%A1%E6%8E%92%E6%9F%A5gRPC%E4%BD%BF%E7%94%A8%E4%B8%8D%E5%BD%93%E5%AF%BC%E8%87%B4%E7%9A%84goroutine%E6%B3%84%E6%BC%8F/</link>
      <pubDate>Fri, 17 Jan 2020 13:20:17 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2020/01/17/%E8%AE%B0%E4%B8%80%E6%AC%A1%E6%8E%92%E6%9F%A5gRPC%E4%BD%BF%E7%94%A8%E4%B8%8D%E5%BD%93%E5%AF%BC%E8%87%B4%E7%9A%84goroutine%E6%B3%84%E6%BC%8F/</guid>
      <description>由于保留必要的“罪证”，因此某些异常只能通过文字来描述了～
 背景 昨晚上10点左右，前端童鞋反映开发环境接口响应超时，但过了几分钟后又恢复了，于是有了这一篇文章。
 其实很久以前就出现了内存占用异常的情况～，只是占用并不高也就是50MB左右，加上当时还忙着写业务需求就没有急着加上pprof来检查。
 首先通过运维平台(k8s based)直观发现了该pod数量从1变成了2, 再结合新增pod的启动时间，我发现该时间正好是前端童鞋反映状况的时间节点，稍后我检查了下该服务的资源限制如下图：
那么前端童鞋反映的问题就很明显了，由于某种原因导致了pod内存超限，触发了运维平台对于内存超限的“容忍机制”。表现为: 新增一个pod用于缓解服务压力，老服务由于无法申请更多内存会导致崩溃或其他异常（无法响应客户端请求），这与反映的情况一致。
pprof排查 知道了服务内存异常，想要具体定位的话，这时候就需要pprof上场了。
 如果你需要重启服务才能开启pprof的话，那么只能等待复现了。这里我在开发环境和测试环境一直开启了pprof，因此可以直接检查。个人觉得，这样还可以帮助开发和测试，完成最初的性能分析😼。
 内存检查 go tool pprof --http=:8080 https://service.host.com/debug/pprof/heap 这个命令是在本地打开一个web服务，直接可视化该服务的内存占用情况。也可以使用:
go tool pprof https://service.host.com/debug/pprof/heap 使用交互模式来分析。通过这个步骤定位到了 grpc相关的包内存占用异常分为两个部分：
50MB+ google.golang.org/grpc/internal/transport.newBufWriter 50MB+ bufio.NewReaderSize http2 相关库的占用也比较多 这一切都指向了我们使用的gRPC，可是为啥使用gRPC会用到这么“多”内存呢？接着分析
goroutine检查 打开一看 https://service.host.com/debug/pprof/一看，goroutine和heap居“高”(4000+)不下，虽然对于动辄10W+的别人家的服务来说，这点根本不算事，但在我们这种小作坊里可就算异常了。点开看goroutine查看详情，有四个部分的goroutine分别有900个左右，这里就算初步定位了“gRPC客户端使用了较多的goroutine，但是却没有正确的结束掉”，如下图（这是解决后的截的图）：
pprof总结 服务中使用的gRPC客户端出了某些故障，导致了goroutine泄漏，引发了OOM（Out Of Memory）。如下图：
代码排查 上一步已经定位到是gRPC客户端的问题，那么就可以直接从代码上手了。我心里已经有一个“嫌疑犯”了，如下：
var ( defaultHandler *handler timeout = 5 * time.Second // _ pb.UserServiceClient = &amp;amp;handler{} ) // Init of usersvc.handler func Init(rpcAddr string) error { // .</description>
    </item>
    
    <item>
      <title>SSH Tunnel小工具</title>
      <link>https://www.yeqown.xyz/2020/01/08/ssh-tunnel%E5%B0%8F%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Wed, 08 Jan 2020 14:33:41 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2020/01/08/ssh-tunnel%E5%B0%8F%E5%B7%A5%E5%85%B7/</guid>
      <description>背景 生产环境数据库不允许直接访问，但是又经常有需要直接操作数据库的需求😂。先不说合不合理，背景就是这个背景，因此只能通过跳板机来连接数据库，一（就）般（我）来（而）说（言）会使用ssh隧道，就轻松能解决这个问题，然鹅，事情并不简单。这里陈述一下：
 生产环境数据库不让直接访问； 跳板机上没有公钥，没有权限； 我一次可能需要开3+个隧道才能启动服务【敲重点】  解决 本着“我不造轮子，谁来造轮子”的想法，这里就造一个小轮子：用Go来实现SSH隧道多开，并支持配置。成果预览： 原理简要分析 如果代理原理有点了解，这里的原理差不多是一样的：Local &amp;lt;-&amp;gt; SSH tunnel &amp;lt;-&amp;gt; Remote Server，对于隧道来说把Local的请求传给Remote, 把Remote的响应告诉Local。直接上代码：
// Start . // TODO: support random port by using localhost:0 func (tunnel *SSHTunnel) Start() error { listener, err := net.Listen(&amp;#34;tcp&amp;#34;, tunnel.LocalAddr) if err != nil { return err } defer listener.Close() // tunnel.Local.Port = listener.Addr().(*net.TCPAddr).Port  for { conn, err := listener.Accept() if err != nil { return err } logger.Infof(tunnel.name() + &amp;#34; accepted connection&amp;#34;) go tunnel.</description>
    </item>
    
    <item>
      <title>数据结构 - hashtable</title>
      <link>https://www.yeqown.xyz/2019/12/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8Bhashtable/</link>
      <pubDate>Thu, 12 Dec 2019 13:50:26 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2019/12/12/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8Bhashtable/</guid>
      <description>背景 最近一直在看《redis设计与实现》，其中讲了redis中使用到的数据结构如：sds, ziplist, skiplist, hashtable, intset, linkedlist等。读完第一部分之后，再结合github上的源码redis，本着好记性不如烂笔头的理念，便准备动手撸一遍。
redis中hashtable的特点  链地址法解决hash冲突（除此以外，常见的冲突解决办法还有：再散列法/再哈希法/建立公共溢出区） 使用了murmur2哈希函数。 渐进式rehash，rehash过程并不是一步到位，而是在get/set/del 等操作中，穿插着完成。 自动扩容和自动收缩，通过阀值来控制扩容和收缩。 有2个bucket，其中0号bucket是最常用的，而1号只会在rehash过程中使用，一旦rehash完成，便不再使用。  解析和实现  hashtable：是根据Key直接访问在内存存储位置的数据结构。如何根据key得到内存中的位置，就需要使用hash函数来从旁协助了。
  hash函数：是一种从任何一种数据中创建小的数字“指纹”的方法。简单的说：hash(input) = 1122334455。
  这里选择了golang来实现；murmur3 hash算法；
 数据结构 一图以蔽之：
// 对外暴露的hashtable type LinkedDict struct { // 2个存储桶，0号正常使用，1号在rehash过程中使用；rehash完成之后，1号赋值给0号然后重置1号。 	ht [2]*hashtable // 初始值 -1，表示没有在rehash 	rehashIdx int } // 存储桶 type hashtable struct { // 底层“数组” 	table []*dictEntry size int sizemask int used int } // hashtable 元素定义 type dictEntry struct { key string value interface{} next *dictEntry } 方法集 hashtable常用的方法有 GET/SET/DELETE/ITER，接下来会在SET和DEL中介绍rehash的详细过程。</description>
    </item>
    
    <item>
      <title>基于socket.io构建即时通讯系统</title>
      <link>https://www.yeqown.xyz/2019/12/04/%E5%9F%BA%E4%BA%8Esocket.io%E6%9E%84%E5%BB%BA%E5%8D%B3%E6%97%B6%E9%80%9A%E8%AE%AF%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Wed, 04 Dec 2019 18:37:31 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2019/12/04/%E5%9F%BA%E4%BA%8Esocket.io%E6%9E%84%E5%BB%BA%E5%8D%B3%E6%97%B6%E9%80%9A%E8%AE%AF%E7%B3%BB%E7%BB%9F/</guid>
      <description>本意在总结实现socket.io-app过程中的一些知识。
 背景 现需要替换公司的即时通信框架（之前用的是阿里云的微消息队列，优点在于：简单易接入，问题在于：对于人数和客户端状态感知不够准确，原因后面细说）。在框架选型的时候，基于应用场景（客户端有：小程序/nodeJS/浏览器），有三种方案：
 替换MQTT的架子，针对现有场景下的问题，选用一款更加可控的MQTT服务，如EMQX。 基于现有的技术栈，选择一款golang开发的开源框架，在此基础上进行开发，如goim。 一个大众且稳定的开源框架，语言不限，如socket.io。  综合一系列因素（技术熟悉程度/star数/时间成本/金钱成本/运维成本）选择了socket.io / JS🐶～
 这里想推荐一下goim给golang程序猿。个人看法：goim对团队技术栈更友好；对分布式更友好；架构合理易于扩展；如有兴趣可以去看下：https://juejin.im/post/5cbb9e68e51d456e51614aab @tsingson 的解析，结合源码更容易理解上手。
  关于socket.io的介绍 文档在此socket.io
通信机制及特点    特点 摘要     可用性 提供 long-polling 和 WebSocket 两种方式，可以自动升级，基于engine.io   自动重连 客户端会一直重连，直到再次链接上服务器   断线检测 客户端和服务端通过心跳维持长链接   二进制消息 任意被序列化的数据结构都可以传输   多路复用 对不同的Namespace,复用底层链接   内置Room概念 理解为聊天室和即可   基于websocket，但不互通 https://github.com/socketio/socket.io-protocol    总结一下：socket.io 已经提供了即时通讯必要的基础，不用关心任何通讯相关的细节，开箱即用。
分布式 确认socket.io已经提供了通讯基础，那么问题来了：单机性能有限，如何扩展到分布式呢? socket.io的另一个优点adapter机制让socket.io易于扩展，官方提供了redis-adapter来支持消息分发。
 By running socket.</description>
    </item>
    
    <item>
      <title>基于Repository设计缓存方案</title>
      <link>https://www.yeqown.xyz/2019/11/14/%E5%9F%BA%E4%BA%8ERepository%E8%AE%BE%E8%AE%A1%E7%BC%93%E5%AD%98%E6%96%B9%E6%A1%88/</link>
      <pubDate>Thu, 14 Nov 2019 09:14:25 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2019/11/14/%E5%9F%BA%E4%BA%8ERepository%E8%AE%BE%E8%AE%A1%E7%BC%93%E5%AD%98%E6%96%B9%E6%A1%88/</guid>
      <description>场景 Tester—A：这个 getInfo 接口咋这么慢呢？查一下要5+s？QPS竟然只有10！！！！ RD-B ：这是因为getInfo要查库。。。N多库 Tester-B：那优化一下呗？ RD-B ：好的，容我操作一波（给接口加上一个响应缓存），好了你再测试一下 Tester-B：（测试中。。。），速度果然快了不少。诶不对，这个接口里拿到的用户信息不对，我明明已经balaba了，这里没有更新！！！ RD-B ：哦哦哦，我晓得咯，再容我操作一波（缓存加有效时间，个人信息更新的时候再强删缓存），O了 至此开始了针对于QPS+缓存更新的一些列测试。。。剧终。  QPS和响应时间是后（jie）端（kou)工程师非常熟悉的指标，这两个值能比较直观的反映该接口的性能，间接直接影响了前端页面的流畅度。。。
问题来了 接口查询性能如何提高 除去机器和编程语言的因素之后，肯定要从业务场景出发，分析接口响应缓慢的原因。譬如，最常见的:
 查N多表，表还没有索引orz 无用数据，增加传输的Size 反复查询某些热点数据，但每次都直接打到数据库 上游服务响应缓慢 其他  好了，这里只讨论热点数据的缓存方案，毕竟要具体场景具体分析，而缓存方案是比较通用的。
缓存方案如何选择    序号 缓存方案 优势 劣势     1 Response缓存 简单暴力 缓存更新时机不好把控，如果面面俱到可能心态崩坏；缓存粒度太大，无法局部更新；针对查询接口有帮助，其他业务下查询数据则毫无帮助   2 Repository缓存 粒度由Repo自行掌握，可控性强；Repo复用场景下会提高应用整体的速度 需要针对各个Repo做缓存的处理；改动较多；其他orz    总的来说，Repository的缓存方案，在上述背景上较简单暴力的中间件缓存法要更加优雅可控～。
缓存算法 提到缓存就一定会提到缓存替换策略，有最常见的：LRU LFU FIFO MRU(最近频繁使用算法) LRU的多个变种算法 LIRS等。 这里选用了LRU-K（K=2）并基于golang来实现 cached-repository，更多算法的详细信息参见参考文档中的LRU和LRU-K:
这里分成了两个interface:
CacheAlgor重点在于与Repo交互，所以只提供了简单的增删改查，底层还是基于Cache来实现的。本意是想实现多种缓存替换算法来丰富cached-repository，orz
// cache.go // CacheAlgor is an interface implements different alg.</description>
    </item>
    
    <item>
      <title>AMQP重连机制实现 Go</title>
      <link>https://www.yeqown.xyz/2019/10/10/AMQP%E9%87%8D%E8%BF%9E%E6%9C%BA%E5%88%B6%E5%AE%9E%E7%8E%B0-Go/</link>
      <pubDate>Thu, 10 Oct 2019 17:41:30 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2019/10/10/AMQP%E9%87%8D%E8%BF%9E%E6%9C%BA%E5%88%B6%E5%AE%9E%E7%8E%B0-Go/</guid>
      <description>文中代码基于 https://github.com/streadway/amqp 实现。此方式简单暴力，但没有做到最小成本迁移（可以选择分别包装Producer和Consumer）。
  文中所有代码参见：https://github.com/yeqown/infrastructure/tree/master/framework/amqp
 基本知识 AMQP AMQP，即Advanced Message Queuing Protocol,一个提供统一消息服务的应用层标准高级消息队列协议，是应用层协议的一个开放标准,为面向消息的中间件设计。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件同产品，不同的开发语言等条件的限制。用下图简单描述下AMQP模型：
背景 -问题 生产环境中使用了RabbitMQ做异步消息分发，隔一段时间会出现：发送接口报错；发送成功后未被消费等情况。重启服务后恢复。
-问题代码 生产者：
// producer.go  // NewClient . func NewClient(cfg *types.RabbitMQConfig) *Client { // init MQ connection 	return &amp;amp;Client{ ch: ch, // *amqp.Channel 	cfg: cfg, } } // Client . type Client struct { ch *amqp.Channel cfg *types.RabbitMQConfig } // Send . send by routing func (c *Client) Send(payload *types.Payload) error { var routing string switch payload.</description>
    </item>
    
    <item>
      <title>TCP拆包粘包</title>
      <link>https://www.yeqown.xyz/2019/09/21/TCP%E6%8B%86%E5%8C%85%E7%B2%98%E5%8C%85/</link>
      <pubDate>Sat, 21 Sep 2019 11:36:13 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2019/09/21/TCP%E6%8B%86%E5%8C%85%E7%B2%98%E5%8C%85/</guid>
      <description>一些名词 MTU（Maximum Transmission Unit）  the maximum transmission unit (MTU) is the size of the largest protocol data unit (PDU) that can be communicated in a single network layer transaction. ——from wiki
 MTU 物理接口（数据链路层）提供给其上层（通常是IP层）最大一次传输数据的大小。一般来说MTU=1500byte。如果MSS + TCP首部 + IP首部 &amp;gt; MTU，那么IP报文就会存在分片，否则就不需要分片。
MSS (Maximum Segment Size)  The maximum segment size (MSS) is a parameter of the options field of the TCP header that specifies the largest amount of data, specified in bytes, that a computer or communications device can receive in a single TCP segment.</description>
    </item>
    
    <item>
      <title>初识Ring Buffer</title>
      <link>https://www.yeqown.xyz/2019/09/19/%E5%88%9D%E8%AF%86Ring-Buffer/</link>
      <pubDate>Thu, 19 Sep 2019 11:10:51 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2019/09/19/%E5%88%9D%E8%AF%86Ring-Buffer/</guid>
      <description>使用场景  一个服务器程序可能会收到多个客户端的网络数据流，在每个数据流上实际上有多个独立的数据包，只有一个数据包接收完整了才能做进一步的处理。如果在一个网络连接上数据包并不完整，就需要暂时缓存住尚未接收完的数据包。
 解决以上场景的方法肯定不止RingBuffer这一种，同理RingBuffer也不只是这一个使用场景，这里只是介绍下RingBuffer在这种场景下的使用。
实现方式 RingBuffer结构定义：
const A = 10 // Byts ... type Byts []byte // RingBuffer . not concurrent safe type RingBuffer struct { data []*Byts pRead uint pWrite uint size uint } // NewRingBuffer . at least 2 func NewRingBuffer(size uint) *RingBuffer { if size &amp;lt;= 1 { panic(&amp;#34;too small size&amp;#34;) } return &amp;amp;RingBuffer{ data: make([]*Byts, size), pRead: 0, pWrite: 0, size: size, } } RingBuffer方法定义：</description>
    </item>
    
    <item>
      <title>LRU和LRU-K</title>
      <link>https://www.yeqown.xyz/2019/08/12/LRU%E5%92%8CLRU-K/</link>
      <pubDate>Mon, 12 Aug 2019 20:10:32 +0800</pubDate>
      
      <guid>https://www.yeqown.xyz/2019/08/12/LRU%E5%92%8CLRU-K/</guid>
      <description>缓存淘汰机制 缓存淘汰机制在缓存需要被清理的时候使用。主要有以下几种算法：
 FIFO：先入先出，优先清理最先被缓存的数据对象。实现简单，直接使用队列就可以实现。 LRU：最近最久未被使用，优先清理最近没有被使用的对象。使用一个最近使用时间降序的有序队列，优先清理队列对后的数据。与LFU的区别在于：LRU是按照最近使用使用的时间排序，LFU需要维护一个使用频次并用于排序。 LFU：最近最少使用，优先清理最近最少使用的数据对象。使用一个使用次数降序的有序队列，优先清理队列最后的数据。  // 其中LRU和LFU可以通过维护一个Hashmap来提高访问效率。
LRU / LRU-1 LRU（Least recently used，最近最少使用）算法根据数据的历史访问记录来进行淘汰数据，其核心思想是“如果数据最近被访问过，那么将来被访问的几率也更高”。实现思路也很简单，就不过多赘述了：
// data to cache type data struct { key int value int } // LRUCache structture to support LRU alg. type LRUCache struct { recoder map[int]*list.Element // key hit for get op O(1)  linked *list.List // linked-list  rest int // rest capacity } // Constructor ... to generate a LRUCache func Constructor(capacity int) LRUCache { c := LRUCache{ rest: capacity, linked: list.</description>
    </item>
    
    <item>
      <title>介绍一下snowflake和rc4</title>
      <link>https://www.yeqown.xyz/2019/04/05/%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8Bsnowflake%E5%92%8Crc4/</link>
      <pubDate>Fri, 05 Apr 2019 14:51:46 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2019/04/05/%E4%BB%8B%E7%BB%8D%E4%B8%80%E4%B8%8Bsnowflake%E5%92%8Crc4/</guid>
      <description>snowflake是twitter公司开源的生成唯一ID的网络服务，具有很强的伸缩性，这里只取用生成唯一ID的算法部分。 rc4（Rivest Cipher 4）是一种流加密算法，密钥长度可变，它的加解密使用相同的密钥，因此也属于对称加密算法。
为啥要介绍这两种算法？ 其一，snowflake可以生成唯一ID，而相比与UUID，snowflake生成的ID更加“好用”，这个放在后面解释。 其二，UUID和snowflake虽然可以生成唯一ID，但是无法适用于所有场景，譬如说“生成推广码”。生成推广码的时候，希望尽可能短而精，很明显唯一ID都不太短。
snowflake snowflake的唯一ID是一个64bit的int型数据，相较于UUID来说耗费空间更小，可以更方便的作为数据库主键来索引和排序。
生成过程：  置0不用 timestamp（41bits）精确到ms。 machine-id（10bits）该部分其实由datacenterId和workerId两部分组成，这两部分是在配置文件中指明的。datacenterId（5bits）方便搭建多个生成uid的service，并保证uid不重复。workerId（5bits）是实际server机器的代号，最大到32，同一个datacenter下的workerId是不能重复的。 sequence-id(12bits)，该id可以表示4096个数字，它是在time相同的情况下，递增该值直到为0，即一个循环结束，此时便只能等到下一个ms到来，一般情况下4096/ms的请求是不太可能出现的，所以足够使用了。  优势和缺陷：  速度快，无依赖，原理和实现简单，也可以根据自己的需求做算法调整 依赖机器时间，如果时间回拨可能导致重复的ID  rc4  RC4加密算法也是一种基于密钥流的加密算法。
 首先，rc4根据明文和密钥生成的密钥流，其长度和明文的长度是相等的，也就是说明文的长度是500字节，那么密钥流也是500字节，这也是我们用来生成推广码的原因之一了；其次，rc4是是对称加密完全可以通过密文得到明文，也就是说在生成码的时候把必要信息放在明文中，在使用密文的时候可以不用查库也能得到相关的信息，譬如用户ID，这是原因之二。
使用场景  现在需要生成一种码，短小易记，且唯一，但并不需要大量。
 上述的snowflake和UUID都很容易实现唯一，但是短小就不符合要求了。因为并不需要大量生成这种码，因此我们考虑用自增ID + RC4来实现：
package main import ( &amp;#34;crypto/rc4&amp;#34; &amp;#34;encoding/hex&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;log&amp;#34; ) func main() { cipher, err := rc4.NewCipher([]byte(&amp;#34;thisiskey&amp;#34;)) if err != nil { log.Fatalf(&amp;#34;wrong with NewCipher: %v&amp;#34;, err) } c := map[string]bool{} for i := 0; i &amp;lt; 1000; i++ { src := []byte(fmt.</description>
    </item>
    
    <item>
      <title>go-watcher-热重载轮子</title>
      <link>https://www.yeqown.xyz/2019/04/01/go-watcher-%E7%83%AD%E9%87%8D%E8%BD%BD%E8%BD%AE%E5%AD%90/</link>
      <pubDate>Mon, 01 Apr 2019 22:19:41 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2019/04/01/go-watcher-%E7%83%AD%E9%87%8D%E8%BD%BD%E8%BD%AE%E5%AD%90/</guid>
      <description>Golang编写的热重载工具，自定义命令，支持监视文件及路径配置，环境变量配置。这是一个重复的轮子～
安装使用 go install github.com/yeqown/go-watcher/cmd/go-watcher 命令行 ➜ go-watcher git:(master) ✗ ./go-watcher -h NAME: go-watcher - A new cli application USAGE: go-watcher [global options] command [command options] [arguments...] VERSION: 2.0.0 AUTHOR: yeqown@gmail.com COMMANDS: init generate a config file to specified postion run execute a command, and watch the files, if any change to these files, the command will reload help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --help, -h show help --version, -v print the version 配置文件 watcher: # 监视器配置 duration: 2000 # 文件修改时间间隔，只有高于这个间隔才回触发重载 included_filetypes: # 监视的文件扩展类型 - .</description>
    </item>
    
    <item>
      <title>goswagger入门手册</title>
      <link>https://www.yeqown.xyz/2019/03/30/goswagger%E5%85%A5%E9%97%A8%E6%89%8B%E5%86%8C/</link>
      <pubDate>Sat, 30 Mar 2019 09:22:55 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2019/03/30/goswagger%E5%85%A5%E9%97%A8%E6%89%8B%E5%86%8C/</guid>
      <description>本文旨在记录使用goswagger过程中遇到的一些问题（只在生成文档方面，不涉及其他功能）：
  如何在go1.11+以上（支持Go Module）版本中的应用swagger 一些注解上的注意事项 如何在团队中管理API文档（主要涵盖了：swagger-ui的部署和使用）  关于swagger swagger涵盖WebAPI的一整套工具：API设计，API实现（代码生成），API文档，API测试及API规范。更多信息请参见官网
准备工作  一个Golang web项目，并软连接到GOPATH/src下。【毕竟是支持Gomodule的项目，还放在GOPATH下就不科学了😄】 安装swagger工具. 参见安装 环境： ➜ swagger-demo git:(master) ✗ go version go version go1.11.5 darwin/amd64 ➜ swagger-demo git:(master) ✗ swagger version version: v0.18.0 commit: 6b23bb61413826ce42c3b14a37bf5870caf91e0b   编写注释  元信息包含了这个应用的基本信息。一般新建一个doc.go放在你的API根目录下；还有一定要注意这句话：
  You give it a main file and it will parse all the files that are reachable by that main package to produce a swagger specification.To use you can add a go:generate comment to your main file。</description>
    </item>
    
    <item>
      <title>QRCode Generator based Golang</title>
      <link>https://www.yeqown.xyz/2018/11/26/QR-Code%E7%94%9F%E6%88%90%E5%99%A8forgolang/</link>
      <pubDate>Mon, 26 Nov 2018 14:07:07 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/11/26/QR-Code%E7%94%9F%E6%88%90%E5%99%A8forgolang/</guid>
      <description>项目地址：yeqown/go-qrcode 同类项目：skip2/go-qrcode 纠错算法和bitset使用了该库，后续可能会考虑自己实现一遍
go-qrcode 示例 link to CODE
package main import ( &amp;#34;fmt&amp;#34; qrcode &amp;#34;github.com/yeqown/go-qrcode&amp;#34; ) func main() { qrc, err := qrcode.New(&amp;#34;https://github.com/yeqown/go-qrcode&amp;#34;) if err != nil { fmt.Printf(&amp;#34;could not generate QRCode: %v&amp;#34;, err) } // save file 	if err := qrc.Save(&amp;#34;../testdata/repo-qrcode.jpeg&amp;#34;); err != nil { fmt.Printf(&amp;#34;could not save image: %v&amp;#34;, err) } } 生成结果如图： QR Code 基本原理 1 数据分析（data analysis）：  分析输入数据，根据数据决定要使用的QR码版本、容错级别和编码模式。低版本的QR码无法编码过长的数据，含有非数字字母字符的数据要使用扩展字符编码模式。
 2 编码数据（data encoding）：  根据选择的编码模式，将输入的字符串转换成比特流，插入模式标识码（mode indicator）和终止标识符（terminator），把比特流切分成八比特的字节，加入填充字节来满足标准的数据字码数要求。</description>
    </item>
    
    <item>
      <title>go-get遇到🧱的解决方法</title>
      <link>https://www.yeqown.xyz/2018/11/19/go-get%E9%81%87%E5%88%B0%E5%A2%99%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</link>
      <pubDate>Mon, 19 Nov 2018 11:33:24 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/11/19/go-get%E9%81%87%E5%88%B0%E5%A2%99%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</guid>
      <description>解决方法有两种，在网上也很好找到：
1. 最简单的，从¥“github.com/golang”找到对应的包并下载到 $GOPATH/src/golang.org/x/ 下 2. 第二种，就是翻过🧱了。 1 问题：翻了🧱，还是没办法直接使用go get来下载呢？ 先说原因，因为go get并没有走你的代理啊！！！！那么如何设置代理呢？
export http_proxy=http://ip:port go get golang.org/xxx 其他设置代理的方式，自行参见 参考
2 如果你的代理不支持http || https协议，可咋整？ 那么想办法支持http或者把http再代理到你可以使用的协议（socks5~），那么可以使用cow。
cow 推荐使用方式：
go get 下载安装（因为刚开始图简单，使用程序的时候运行报错了，go get 安装方式并不会有这样的困扰）。 配置的时候也很简单，编辑配置文件 ～/.cow/rc，配置http socks5代理服务和监听代理端口。
listen = http://127.0.0.1:7777 proxy = socks5://127.0.0.1:1080 3 运行 配置完成之后就可以直接运行了
 ./cow # 另开一个Terminal export http_proxy=http://ip:port go get golang.org/xxx 写在后面 知道了go get无法翻🧱的原因之后，就可以发挥自己的想象力来解决问题了，这样解决还是挺繁琐的。虽然cow可以配置开机启动，但对于一个懒癌晚期）的人来说（如果不是因为升级Go到了1.11，go-module机制让我无法开心的玩耍，我也不会去考虑为啥翻🧱了go get还是不能用，明明有代理却还有使用另外一个代理～。
友情提示：vscode-go 也可以设置proxy哦
参考：  go get 使用代理 cow cow配置文件说明 vscode-go设置proxy  </description>
    </item>
    
    <item>
      <title>etcd与service-registration-discovery</title>
      <link>https://www.yeqown.xyz/2018/10/23/etcd-service-registration-discovery/</link>
      <pubDate>Tue, 23 Oct 2018 17:20:48 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/10/23/etcd-service-registration-discovery/</guid>
      <description>声明：本文对etcd的原理，实现细节，性能等均不考虑，仅将etcd作为一个分布式的K-V存储组件。本文提价代码均在： github.com/yeqown/server-common/tree/master/framework/etcd
 一个核心 etcd, 分布式Key-Value存储工具。详细资料由此去
 etcd下载安装  两个对象  服务提供者（在测试环境中，我定义为单独的服务实例），也就是服务的提供者，需要向其他服务暴露自己的ip和端口，方便调用。 服务调用者（同样地，在测试环境中我定义为反向代理网关程序），也就是服务的调用者，需要获取到 可使用 地服务地址并调用。  关于服务注册与发现 就具体场景而言：我们的生产环境中使用了一个代理网关服务器，用于转发移动端和PC端的API请求，并完成其他功能。所有的服务实例配置都是硬编码在网关程序中，顶多就是抽离出来成了一个配置文件。这样做的缺点很明显：“非动态”。也就意味着，一旦有服务Down掉，那么用户访问则可能异常，甚至导致整个服务的崩溃；其次，需要对服务进行扩容的情况下，则需要先进行服务部署再更新网关程序，步骤繁琐且容易出错。
那么如果我们设计成为如下图的样子： 对于新添加的服务实例，只需要启动新的服务，并注册到etcd相应的路径下就行了。
 注册：对于同一组服务，配置一个统一的前缀（如图上的&amp;quot;/specServer&amp;quot;），不同实例使用ID加以区分。
 将现行服务改造成为上述模式需要解决的问题：  etcd 配置安装 网关程序改造（监听etcd的节点夹子/prefix;适配动态的服务实例调用） 服务实例改造（注册服务实例到etcd;心跳更新;其他配套设施，异常退出删除注册信息）  etcd安装配置在github.com已经非常详细了。在这里贴一下我在本地测试时候启动的脚本（这部分是从etcd-demo获取到的，做了针对端口的改动）：
#!/bin/bash  # For each machine TOKEN=token-01 CLUSTER_STATE=new NAME_1=machine1 NAME_2=machine2 NAME_3=machine3 HOST_1=127.0.0.1 HOST_2=127.0.0.1 HOST_3=127.0.0.1 CLUSTER=${NAME_1}=http://${HOST_1}:2380,${NAME_2}=http://${HOST_2}:2381,${NAME_3}=http://${HOST_3}:2382 # For machine 1 THIS_NAME=${NAME_1} THIS_IP=${HOST_1} etcd --data-dir=machine1.etcd --name ${THIS_NAME} \ 	--initial-advertise-peer-urls http://${THIS_IP}:2380 --listen-peer-urls http://${THIS_IP}:2380 \ 	--advertise-client-urls http://${THIS_IP}:2377 --listen-client-urls http://${THIS_IP}:2377 \ 	--initial-cluster ${CLUSTER} \ 	--initial-cluster-state ${CLUSTER_STATE} --initial-cluster-token ${TOKEN} &amp;amp; # For machine 2 THIS_NAME=${NAME_2} THIS_IP=${HOST_2} etcd --data-dir=machine2.</description>
    </item>
    
    <item>
      <title>Golang适用的DTO工具</title>
      <link>https://www.yeqown.xyz/2018/08/29/Golang%E9%80%82%E7%94%A8%E7%9A%84DTO%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Wed, 29 Aug 2018 09:27:36 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/08/29/Golang%E9%80%82%E7%94%A8%E7%9A%84DTO%E5%B7%A5%E5%85%B7/</guid>
      <description>DTO (Data Transfer Object) 是Java中的概念，起到数据封装和隔离的作用。在使用Golang开发Web应用的过程中，也会有类似的需求。先贴项目地址 github.com/yeqown/server-common/tree/master/dbs/tools
 举个例子 现在有一个用户数据结构如下,
type UserModel struct { ID int64 `gorm:&amp;#34;column:id&amp;#34;` Name string `gorm:&amp;#34;column:name&amp;#34;` Password string `gorm:&amp;#34;column:password&amp;#34;` } // 问题1: 现在要求是想要JSON格式返回用户数据，并且不希望其中包含有Password字段 // 解决1:
type UserModel struct { ID int64 `gorm:&amp;#34;column:id&amp;#34; json:&amp;#34;id&amp;#34;` Name string `gorm:&amp;#34;column:name&amp;#34; json:&amp;#34;name&amp;#34;` Password string `gorm:&amp;#34;column:password&amp;#34; json:&amp;#34;-&amp;#34;` } // 问题2: 同样是JSON数据格式，并且希望额外返回用户的身份标示Ident（假设必须要跟用户数据放在一起） // 解决2: （这也是我的场景）
type UserDTO struct { ID int64 `json:&amp;#34;id&amp;#34;` Name string `json:&amp;#34;name&amp;#34;` Password string `json:&amp;#34;-&amp;#34;` Ident string `json:&amp;#34;ident&amp;#34;` } func LoadUserDTOFromModel(data *UserMolde) *UserDTO { ident := genUserIdent(data) return &amp;amp;{ ID data.</description>
    </item>
    
    <item>
      <title>怎么才叫熟悉http协议?</title>
      <link>https://www.yeqown.xyz/2018/06/28/%E6%80%8E%E4%B9%88%E6%89%8D%E5%8F%AB%E7%86%9F%E6%82%89http%E5%8D%8F%E8%AE%AE/</link>
      <pubDate>Thu, 28 Jun 2018 14:50:08 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/06/28/%E6%80%8E%E4%B9%88%E6%89%8D%E5%8F%AB%E7%86%9F%E6%82%89http%E5%8D%8F%E8%AE%AE/</guid>
      <description>“熟悉http协议”，肯定很多IT小伙伴都在招聘岗位上看得到过，但是怎么才叫熟悉http协议呢？抽空梳理了一下，也算是对这一部分知识的笔记吧！
可能对于大部分人来说，网络web编程就是使用一些第三方库来进行请求和响应的处理，再多说一点就是这个URI要使用POST方法，对于携带的数据需要处理成为formdata。
基础知识 Q1: HTTP协议是什么？用来干什么？
 HTTP协议是基于TCP/IP协议的应用层协议，主要规定了客户端和服务端之间的通信格式。主要作用也就是传输数据（HTML，图片，文件，查询结果）。
 #网络分层  互联网的实现分成了几层，如何分层有不同的模型（七层，五层，四层），这里按五层模型来解释：
 （靠近用户）应用层 &amp;lt; 传输层 &amp;lt; 网络层 &amp;lt; 链接层 &amp;lt; 物理层（靠近硬件）
   层级 作用 拥有协议     物理层 传送电信号0 1 无   数据链路层 定义数据包;网卡MAC地址;广播的发送方式; Ethernet 802.3; Token Ring 802.5   网络层 引进了IP地址，用于区分不同的计算机是否属于同一网络 IP; ARP; RARP   传输层 建立端口到端口的通信，实现程序时间的交流，也就是socket TCP; UDP   应用层 约定应用程序的数据格式 HTTP; FTP; DNS    每一层级，都是解决问题而诞生的，也就是他们各自作用对应的问题，推荐参考资料中的“互联网协议入门”。
#HTTP通信流程 #拓展&amp;ndash;三次握手和四次挥手 经常在其他地方看到这些，一直不知道了解这部分有什么用，但是syn Flood攻击，恰恰是利用了TCP三次握手中的环节。利用假IP伪造SYN请求，服务端会多次尝试发送SYN-ACK给客户端，但是IP并不存在也就无法成功建立连接。在一定时间内伪造大量这种请求，会导致服务器资源耗尽无法为正常的连接服务。(注：服务器SYN连接数量有限制，SYN-ACK超时重传机制)</description>
    </item>
    
    <item>
      <title>api-gateway中实现基于权重的轮询调度</title>
      <link>https://www.yeqown.xyz/2018/06/08/api-gateway%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8E%E6%9D%83%E9%87%8D%E7%9A%84%E8%BD%AE%E8%AF%A2%E8%B0%83%E5%BA%A6/</link>
      <pubDate>Fri, 08 Jun 2018 10:53:32 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/06/08/api-gateway%E4%B8%AD%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8E%E6%9D%83%E9%87%8D%E7%9A%84%E8%BD%AE%E8%AF%A2%E8%B0%83%E5%BA%A6/</guid>
      <description>背景和目标 背景 项目需要在现有项目的基础上实现权限系统，但为了低耦合，选择实现了一个基于ne7ermore/gRBAC的auth-server，用于实现权限，角色，用户的管理，以及提供鉴权服务。在开发环境对接没有问题，正常的鉴权访问。到了线上部署的时候，才发现：
 线上某服务部署在多台机器上; 目前的api-gateway并不支持同一服务配置多个node;  想的办法有：
   序号 描述 优点 缺点     1 api-gateway通过url来转发请求，之前是配置IP加端口 api-gateway改动小 影响web和APP升级   2 api-gateway能支持多台机器，并进行调度 api-gateway功能更强大，把以后要做的事情提前做好基础 好像没啥缺点，只是费点时间支持下多节点配置，并调度     如果没说清，请看下图：
 目标 那么，目标也就明确了，需要实现api-gateway中实现基于权重的调度。为啥要基于权重？其一是仿照nginx基于权重的负载均衡，其二是服务器性能差异。
轮询调度算法介绍 轮询调度算法:  轮询调度算法的原理是每一次把来自用户的请求轮流分配给内部中的服务器，从1开始，直到N(内部服务器个数)，然后重新开始循环。该算法的优点是其简洁性，它无需记录当前所有连接的状态，所以它是一种无状态调度。
 假设有一组服务器N台，S = {S1, S2, …, Sn}，一个指示变量i表示上一次选择的服务器ID。变量i被初始化为N-1。其算法如下：
j = i; do { j = (j + 1) mod n; i = j; return Si; } while (j != i); return NULL; 平滑加权轮询调度算法：  上述的轮询调度算法，并没有考虑服务器性能的差异，实际生产环境中，每一台服务器配置和安装的业务并不一定相同，处理能力不完全一样。因此需要根据服务器能力，分配不同的权值，以免服务的超负荷和过分闲余。</description>
    </item>
    
    <item>
      <title>使用golang 实现JSON-RPC2.0</title>
      <link>https://www.yeqown.xyz/2018/05/18/%E4%BD%BF%E7%94%A8golang-%E5%AE%9E%E7%8E%B0JSON-RPC2-0/</link>
      <pubDate>Fri, 18 May 2018 17:52:18 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/05/18/%E4%BD%BF%E7%94%A8golang-%E5%AE%9E%E7%8E%B0JSON-RPC2-0/</guid>
      <description>什么是RPC？ 远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。如果涉及的软件采用面向对象编程，那么远程过程调用亦可称作远程调用或远程方法调用。
远程过程调用是一个分布式计算的客户端-服务器（Client/Server）的例子，它简单而又广受欢迎。远程过程调用总是由客户端对服务器发出一个执行若干过程请求，并用客户端提供的参数。执行结果将返回给客户端。由于存在各式各样的变体和细节差异，对应地派生了各式远程过程调用协议，而且它们并不互相兼容。——————源自维基百科
 什么又是JSON-RPC? JSON-RPC，是一个无状态且轻量级的远程过程调用（RPC）传送协议，其传递内容通过 JSON 为主。相较于一般的 REST 通过网址（如 GET /user）调用远程服务器，JSON-RPC 直接在内容中定义了欲调用的函数名称（如 {&amp;ldquo;method&amp;rdquo;: &amp;ldquo;getUser&amp;rdquo;}），这也令开发者不会陷于该使用 PUT 或者 PATCH 的问题之中。 更多JSON-RPC约定参见：https://zh.wikipedia.org/wiki/JSON-RPC
问题 服务端注册及调用 约定如**net/rpc**：
  the method&amp;rsquo;s type is exported.    the method is exported. the method has two arguments, both exported (or builtin) types. the method&amp;rsquo;s second argument is a pointer. the method has return type error.  // 这就是约定 func (t *T) MethodName(argType T1, replyType *T2) error 那么问题来了:</description>
    </item>
    
    <item>
      <title>自己写一个手机菜谱APP</title>
      <link>https://www.yeqown.xyz/2018/05/18/%E8%87%AA%E5%B7%B1%E5%86%99%E4%B8%80%E4%B8%AA%E6%89%8B%E6%9C%BA%E8%8F%9C%E8%B0%B1APP/</link>
      <pubDate>Fri, 18 May 2018 12:38:14 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/05/18/%E8%87%AA%E5%B7%B1%E5%86%99%E4%B8%80%E4%B8%AA%E6%89%8B%E6%9C%BA%E8%8F%9C%E8%B0%B1APP/</guid>
      <description>需要的技术及工具：
 Python3 + Selenuium Golang net/http React-Native 相关（使用了react-navigation） MongoDB Redis  代码地址：
 github.com/yeqown/recipe  项目构思及构成 食谱类型的App，应用市场肯定有更好的的食谱APP。所以自己开发的目的，首先是写代码，其次是定制APP～ 好的，现在化身产品经理，设计一下APP有哪些功能：
 每日菜谱推荐，推荐可更换 每天需要准备的材料提醒 发现更多菜谱 分类筛选菜谱 搜索菜谱 查看菜谱详情 设置（不知道设置啥，提前准备吧）   设计稿？不存在的，随心所欲。
 现在分析下我需要做的事情：
 能跑起来的APP，与restful web api 交互。 能跑起来的web-api，提供菜谱数据，筛选，推荐，搜索等功能 能跑起来的简易spider，从网上获取菜谱信息。（这个爬虫能解析动态生成网站就够用了，姑且称之为爬虫吧）   没有考虑大量数据，因此爬虫并不通用，只适合特定XX网站。
 实战爬虫 这个APP里面最重要的就是菜谱数据了，那么开发之前，需要明确的数据格式，如下：
{ &amp;#34;name&amp;#34;: &amp;#34;name&amp;#34;, &amp;#34;cat&amp;#34;: &amp;#34;cat&amp;#34;, &amp;#34;img&amp;#34;: &amp;#34;img_url&amp;#34;, &amp;#34;mark_cnt&amp;#34;: 19101, &amp;#34;view_cnt&amp;#34;: 181891, &amp;#34;setps&amp;#34;: [ { &amp;#34;desc&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;img&amp;#34;: &amp;#34;&amp;#34;, }, // more step ], &amp;#34;material&amp;#34;: { &amp;#34;ingredients&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;ingredients_name&amp;#34;, &amp;#34;weight&amp;#34;: &amp;#34;ingredients_weight&amp;#34;, }, // more ingredients ], &amp;#34;seasoning&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;seasoning_name&amp;#34;, &amp;#34;weight&amp;#34;: &amp;#34;seasoning_weight&amp;#34;, }, // more seasoning ], }, &amp;#34;create_time&amp;#34;: &amp;#34;2018xxxxxx&amp;#34;, &amp;#34;update_time&amp;#34;: &amp;#34;2018xxxxxx&amp;#34;, } 目标  前提：无法直接获取到该网站的服务API，才使用爬虫间接获取数据。</description>
    </item>
    
    <item>
      <title>docker&#43;jenkins&#43;golang持续集成实践</title>
      <link>https://www.yeqown.xyz/2018/05/03/docker-jenkins-golang%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Thu, 03 May 2018 16:54:10 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/05/03/docker-jenkins-golang%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90%E5%AE%9E%E8%B7%B5/</guid>
      <description>起因 因为生产需要最近又重新折腾了一下Jenkins和docker。主要目的是想自动编译，打包，部署一些Golang的HttpServer。于是决定使用Jenkins来做这个持续集成的载体，选择Jenkins出于两点原因：
1. 以前就使用过，上手会更快 2. 社区比较成熟，插件和文档丰富
 安装Docker和Pull Jenkins镜像 这一步，作为前置条件且不是本文主要要描述的步骤，因此略去。网上也有很多参考资料～
 Jenkins &amp;amp; docker-compose配置 为了方便我才用了docker-compose这个工具，docker-compose 基础可以参见我的docker-compose上手。这里直接上配置：
version: &amp;#39;2&amp;#39; services: jenkins: container_name: jenkins-lts ports: - 9001:8080 - 50000:50000 image: jenkins/jenkins:lts volumes: - /home/worker/jenkins/jenkins_home:/var/jenkins_home 配置也是官方的示例配置。
 Note: 将宿主机的/home/worker/jenkins/jenkins_home挂载为容器的/var/jenkins_home目录。这样做的目的是，如果容器被不小心删除也不至于Jenkins的数据丢失。
 到这里，我们只需要执行docker-compose up -d便可以将Jenkins容器跑起来了，再配置一下Nginx，便可以直接访问到Jenkins页面了，并进行初始化。
我的目录结构如下：
➜ jenkins ll total 8.0K -rw-rw-r-- 1 worker worker 220 May 2 17:19 docker-compose.yml drwxrwxr-x 19 worker worker 4.0K May 3 15:53 jenkins_home ➜ jenkins pwd /home/worker/jenkins ➜ jenkins docker-compose up -d # 运行   Publish Over SSH配置 Publish Over SSH配置，由于我们是通过docker运行的Jenkins，因此要特别配置一下SSH，方便Jenkins部署项目。这里先列出步骤：</description>
    </item>
    
    <item>
      <title>gorm使用记录</title>
      <link>https://www.yeqown.xyz/2018/04/20/gorm%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/</link>
      <pubDate>Fri, 20 Apr 2018 16:23:38 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/04/20/gorm%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95/</guid>
      <description>关于Gorm gorm文档
遇见问题 无法通过结构体的方式更新或查询零值 这里零值是说，各个类型的默认值。  关于这一点是在这里中注明了的，也提供了解决方案：
 WARNING when update with struct, GORM will only update those fields that with non blank value
  For below Update, nothing will be updated as &amp;ldquo;&amp;rdquo;, 0, false are blank values of their types
  NOTE When query with struct, GORM will only query with those fields has non-zero value, that means if your field’s value is 0, &amp;lsquo;&amp;rsquo;, false or other zero values, it won’t be used to build query conditions,</description>
    </item>
    
    <item>
      <title>分布式架构入门</title>
      <link>https://www.yeqown.xyz/2018/04/17/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%85%A5%E9%97%A8/</link>
      <pubDate>Tue, 17 Apr 2018 15:01:33 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/04/17/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%85%A5%E9%97%A8/</guid>
      <description>在开始之前必须明确的是，分布式和集群的区别，简单的说：
1.分布式是一种工作方式，把一个系统的不同功能放在不同的机器上； 2.集群是一种物理形态，同一个任务放在不同的机器上； 这样说，也并不是说这两个概念是完全不同，还互相独立，而是实际应用中相辅相成。  我们常说的负载均衡的背后就是集群部署，某些公司为了能扛住突然增长的流量，会采用加很多台服务器的方式来提高性能。看下图： 对于小公司来说，这样部署的方式在于：简单快速，成本在可接受范围。（如果愿意多花点时间进行代码重构和技术再选型或许效果会更好，当然估计要换个不懂技术的老板，然后忽悠他😊）
但是也不是没有隐患，一方面，集群部署虽然能提高系统的可用性，但是如果多台机器离线，会导致其他机器压力增大，如果严重超过机器负载能力，会导致越来越多的机器离线，一旦解决不及时便会导致整个应用崩溃。其次，集群部署为了保证数据的一致性，一般多采用相同数据源，因此集群并不能无限制扩张。
分布式系统的应用场景 分布式主要解决的问题是提升应用的负载能力。
分布式的优缺点 将一个系统的不同模块分别部署在不同的机器上。这里不得不说到微服务，微服务是把系统服务拆分成为独立的服务来部署（这里说的独立，是数据独立和部署独立，不依赖于其他服务）。
从本质上来说微服务也是分布式部署的一种。只是相比一般分布式应用，拆分更加彻底。
优点 分成多个模块，并使用接口通信，低耦合； 团队协作开发，事先约定好接口，独立开发，效率更高； 部署更加灵活；  缺点 依赖网络通信，增加额外开发通信接口。  总结 以上说的都是垃圾，只希望打开思路。某邓同志说：能抗住压力的应用才是好应用。
参考资料 https://blog.csdn.net/boonya/article/details/55046568 https://www.jianshu.com/p/39c1e4ec0d63</description>
    </item>
    
    <item>
      <title>golang热重载工具</title>
      <link>https://www.yeqown.xyz/2018/04/17/golang%E7%83%AD%E9%87%8D%E8%BD%BD%E5%B7%A5%E5%85%B7/</link>
      <pubDate>Tue, 17 Apr 2018 14:05:09 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/04/17/golang%E7%83%AD%E9%87%8D%E8%BD%BD%E5%B7%A5%E5%85%B7/</guid>
      <description>运行截图 项目地址 https://github.com/yeqown/gw
本项目由https://github.com/silenceper/gowatch改造而来。不同之处在于:
本gw任意指定重新执行的命令。 而原来的gw默认是重新编译打包go程序。</description>
    </item>
    
    <item>
      <title>Golang学习笔记</title>
      <link>https://www.yeqown.xyz/2018/04/08/Golang%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sun, 08 Apr 2018 15:01:33 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/04/08/Golang%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</guid>
      <description>目录  Channel Goroutine  Channel 一开始是在看channel的源码，结果发现里面含有一些抽象的描述（可能也就是我觉得。。。毕竟没有深入）
 Do not change another G&amp;rsquo;s status while holding this lock (in particular, do not ready a G), as this can deadlock with stack shrinking.
 其中G是啥？我看着是很懵逼的，去google了一下，其实是goroutine相关的知识，那就把goroutine理解了先。
 2020-04-13 填坑
 channel in go
Goroutine  G: 表示goroutine，存储了goroutine的执行stack信息、goroutine状态以及goroutine的任务函数等；另外G对象是可以重用的。 P: 表示逻辑processor，P的数量决定了系统内最大可并行的G的数量（前提：系统的物理cpu核数&amp;gt;=P的数量）；P的最大作用还是其拥有的各种G对象队列、链表、一些cache和状态。 M: 代表着真正的执行计算资源。在绑定有效的p后，进入schedule循环；而schedule循环的机制大致是从各种队列、p的本地队列中获取G，切换到G的执行栈上并执行G的函数，调用goexit做清理工作并回到m，如此反复。M并不保留G状态，这是G可以跨M调度的基础。M必须关联了P才能执行Go代码。
 结合下图更方便理解： &amp;ndash;源于Tonybai的博客，见参考资料。 参考资料  runtime/runtime2.go Tonybai-goroutine调度器  </description>
    </item>
    
    <item>
      <title>aliyun-rds数据备份方案</title>
      <link>https://www.yeqown.xyz/2018/03/02/aliyun-rds%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD%E6%96%B9%E6%A1%88/</link>
      <pubDate>Fri, 02 Mar 2018 17:52:53 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/03/02/aliyun-rds%E6%95%B0%E6%8D%AE%E5%A4%87%E4%BB%BD%E6%96%B9%E6%A1%88/</guid>
      <description>本文主要是总结下在使用aliyun-rds数据备份方案过程中的心得。
高可用一直都是线上服务维护用户体验的关键之一。为了达到高可用，业界已经有了很多方案。最典型的就是“冗余备份+自动故障转移”。冗余备份是说，当一个节点服务不可用时，有其他服务能够代替其工作。除此之外，如果服务出现了必须人工介入解决的故障，也会影响系统的高可用特性。
 本文着重介绍数据的高可用方案
 数据库冗余 如果是单节点的数据库，还用的着说吗？要保证服务高可用，除了主-从数据库之外，还需要从备份数据库，当然不能保证说一定不会遇到所有的备份数据库，都挂掉的情况&amp;hellip;。阿里云提供了RDS-高可用版本和RDS-单机版,两者的区别见下图： 这就算最基本的冗余了，没有主从复制，没有读写分离。但是能保证主库在换掉的时候，还能使用备库提供服务。如果服务对于数据库性能和可用性有一定要求，那么可以在这个基础上升个级，见下图： 数据故障自动转移 已经有了冗余的数据库节点了，那么接下来要做的事情就是怎么感知数据库异常，并实现自动切换到备份实例中? 阿里云灾备方案的文档是这样描述的：
 主实例和灾备实例均搭建主备高可用架构，当主实例所在区域发生突发性自然灾害等状况，主节点（Master）和备节点（Slave）均无法连接时，可将异地灾备实例切换为主实例，在应用端修改数据库链接地址后，即可快速恢复应用的业务访问。
 对于主节点全部不可用的情况对应用服务是可见的，因此应用服务可以通过指定一些异常判断，在判定主节点不可用的时候，主动切换数据库连接地址来获取数据，提供服务。
// sql-detect.go  package main import ( &amp;#34;database/sql&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;sync&amp;#34; &amp;#34;time&amp;#34; _ &amp;#34;github.com/go-sql-driver/mysql&amp;#34; _ &amp;#34;github.com/mxk/go-sqlite&amp;#34; ) var ( mysqlAvailable bool = true mutex = sync.Mutex{} db *sql.DB = nil ) func MysqlDetection(db *sql.DB, ticker *time.Ticker) { for { select { case &amp;lt;-ticker.C: if e := db.Ping(); e != nil { fmt.Println(&amp;#34;got error&amp;#34;, e) mutex.Lock() mysqlAvailable = false mutex.</description>
    </item>
    
    <item>
      <title>Stack实现O(1)的Min和Max</title>
      <link>https://www.yeqown.xyz/2018/03/01/Stack%E5%AE%9E%E7%8E%B0O1%E7%9A%84Min%E5%92%8CMax/</link>
      <pubDate>Thu, 01 Mar 2018 16:26:05 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/03/01/Stack%E5%AE%9E%E7%8E%B0O1%E7%9A%84Min%E5%92%8CMax/</guid>
      <description>栈（Stack）Pop和Push操作只需要对栈顶元素进行操作，就不多加描述了。那么对于Max和Min操作，怎么保证O(1)的时间复杂度?最直接想到的就是设置两个标记位，最小值的最大值，在push和pop的时候更新两个值。那么怎么更新呢，怎么保证最大值和最小值弹出之后还能正确获取到当前所有元素中的最大值和最小值呢？请看下文：
辅助最大值栈SM 算法描述 type Stack Struct{ data []int } var SMax *Stack = new(Stack)  push: 如果当前元素大于等于辅助栈的栈顶元素或者辅助栈为空，那么当前元素push到辅助栈中 pop: 如果当前元素等于辅助栈的栈顶元素，那么从辅助栈中弹出当前元素  举个例子 如果有1，3，6，1，12，512，12，5121，121，412数据放入栈中  Step-1. 元素1入栈，当前SM栈为空，SM栈也同步更新
Stack: 1 SMax: 1  Step-2. 元素3入栈，3 &amp;gt; 1，SMax栈也同步更新
Stack: 1, 3 SMax: 1, 3  Step-3. 元素6入栈，6&amp;gt;3，SMax栈也同步更新
Stack: 1, 3, 6 SMax: 1, 3, 6  &amp;hellip;此处省略更多步骤
最大值标记法 第一种方式利用辅助栈来标记当前最大值和上一个最大值，并利用栈来实现O(1)复杂度。但是根据上述的例子，可以看到如果插入的元素是依次增大，那么耗费2N+1空间才能实现栈的最大值和最小值在O(1)复杂度。现在介绍的方法，能够很好的减少空间耗费，并保证O(1)时间复杂度。
算法描述 type Stack struct { data []int max int // default = math.MinInt32 }  push: 将(当前元素-Max)放入栈中；如果当前元素大于Max，用当前元素替换Max pop: 如果栈顶元素&amp;gt;0，弹出Max，用Max-栈顶元素替换Max；否则弹出Max+栈顶元素  再举个例子 如果有5, 23, 12, 499, 45, 20, 60入栈  Step-1.</description>
    </item>
    
    <item>
      <title>Trie树学习</title>
      <link>https://www.yeqown.xyz/2018/02/11/Trie%E6%A0%91%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Sun, 11 Feb 2018 15:03:31 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/02/11/Trie%E6%A0%91%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/</guid>
      <description>Trie树(Retrieval Tree)又称前缀树，可以用来保存多个字符串，并且查找效率高。在trie中查找一个字符串的时间只取决于组成该串的字符数，与树的节点数无关。Trie树形状如下图： 应用场景  词频统计（搜索引擎常用） 前缀单词搜索  构造Trie树 构造Trie树有如下几种方式（非全部）：
// 结构1，简单且直观，但是空间闲置较多，利用率低下 type TrieNode struct{ Char	rune Children [27]*TrieNode } // 结构二 可变数组Trie树, 减少了闲置指针，但是只能通过遍历来获取下一状态，降低了查询效率 type TrieNode struct { Char rune Children []*TrieNode } // 结构3，双数组Trie树，空间和时间上耗费较为均衡，但是动态构建，解决冲突耗费时间较多 type TrieNode struct { Base []int Check []int } 数组构造方式 这里选择双数组方式来实现Trie树。
 基于数组的实现方式，把trie看作一个DFA，树的每个节点对应一个DFA状态，每条从父节点指向子节点的有向边对应一个DFA变换。遍历从根节点开始，字符串的每个字符作为输入用来确定下一个状态，直到叶节点。 &amp;mdash;- 摘自参考资料，Trie数组实现原理
 关于双数组：
 Base数组，表示后即节点的基地址的数组，叶子节点没有后继 Check数组，用于检查  Trie树应用之前缀搜索 前缀搜索。也就是给一定的字符串，给出所有以该字符串开始的单词。譬如，Search(&amp;ldquo;go&amp;rdquo;)，得到[&amp;ldquo;go&amp;rdquo;, &amp;ldquo;golang&amp;rdquo;, &amp;ldquo;google&amp;rdquo;, &amp;hellip;]
三种构造方式的优劣分析 Trie树（数组Trie树） 每个节点都含有26个字母指针，但并不是都会使用，内存利用率低。时间复杂度：O(k)， 空间复杂度：O(26^n)
双数组Trie树 构造调整过程中，每个状态都依赖于其他状态，所以当在词典中插入或删除词语的时候，往往需要对双数组结构进行全局调整,灵活性能较差。双数组已经大幅改善了经典Trie树的空间浪费，但是冲突发生的时候，总是往后寻址，不可避免数组空置。随着数据量增大，冲突的几率也越来越大，字典树的构建也越来越慢。如果核心词典已经预先建立好并且有序的，并且不会添加或删除新词，那么这个缺点是可以忽略的。所以常用双数组Tire树都是载入整个预先建立好的核心分词词典。
Tail-Trie树 三数组Trie树实在双数组的基础上优化而来，增加了tail节点，就是将非公共前缀的词尾合并成为一个节点，减少节点总数，提升词典树的构建速度。如图： 参考资料  An Implementation of Double-Array Trie Trie树详解及应用 DoubleArrayTrie An Efficient Implementation of Trie Structures github.</description>
    </item>
    
    <item>
      <title>动态规划之字符串编辑距离</title>
      <link>https://www.yeqown.xyz/2018/02/11/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB%E9%97%AE%E9%A2%98-LD/</link>
      <pubDate>Sun, 11 Feb 2018 09:57:18 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/02/11/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E4%B9%8B%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB%E9%97%AE%E9%A2%98-LD/</guid>
      <description>问题描述 给定 2 个字符串 a, b. 编辑距离是将 a 转换为 b 的最少操作次数，操作只允许如下 3 种： 插入一个字符，例如：fj -&amp;gt; fxj 删除一个字符，例如：fxj -&amp;gt; fj 替换一个字符，例如：jyj -&amp;gt; fyj
函数原型：
func LevenshteinDis(str1, str2 string) int { ... } 算法适用场景  拼写检查 输入联想 语音识别 论文检查 DNA分析  问题分析 假定函数edit_dis(stra, strb)表示，stra到strb的编辑距离。算法问题可以分为四种情况：
 edit_dis(0, 0) = 0 edit_dis(0, strb) = len(strb) edit_dis(stra, strb) = len(stra) edit_dis(stra, strb) = ?  对于4th一般情况，没有办法直接给出求解方式，我们来分析edit_dis(stra+chara, strb+charb)可能的情况：
 stra能转成strb，那么只需要判断chara是不是等于charb (cur_cost = 0 if chara == charb else 1) stra+chara能转成strb, 那么要让stra + chara 转成strb+ charb, 只需要插入charb就行了 如果stra 可以直接转成strb+charb，那么删除chara就可以转换成功了  综上的分析，可以得到如下DP公式：</description>
    </item>
    
    <item>
      <title>ShortURL系统实现</title>
      <link>https://www.yeqown.xyz/2018/01/29/ShortURL%E7%94%9F%E6%88%90%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Mon, 29 Jan 2018 10:22:00 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/01/29/ShortURL%E7%94%9F%E6%88%90%E7%B3%BB%E7%BB%9F/</guid>
      <description>在知乎上看了一个很有启发的回答，因此实际动手来实现短URL生成系统。贴上链接： 知乎 - 短URL系统是如何设计的。其中提到了，要实现短URL生成系统要解决的问题有：
 如何优雅的实现？ 怎么基本实现长对短、一对一？ 如何实现分布式，高并发，高可用？ 储存选用？  基本原理 数据库自增ID转换62进制
 使用自增ID不会产生重复的短链接。 为了解决自增ID超长和不便记忆，对ID进行62进制编码。所谓62进制就是0-9，a-z，A-Z。  简单计算下：
62 ^ 4 = 14,776,336 62 ^ 5 = 916,132,832 62 ^ 6 = 56,800,235,584 // 已经足够使用了 总体结构及处理流程 长链接处理流程  获取参数，调用shortURL服务 尝试从缓存中获取，如果命中，则读取短链接(重置过期时间)。跳转第4步 将长链接存储到Mysql数据库，根据ID进行base62编码，组装Domain+Encoded字符串并更新数据库 返回生成的短链接  短链接处理流程  解析短链接为ID 查询ID对应的长链接 以301方式跳转到长链接  长链接与短链接的对应关系 一对多，一个长链接可能对应多个短链接。数据表存储结构如下：
+-----------+--------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-----------+--------------+------+-----+---------+----------------+ | id | int(64) | NO | PRI | NULL | auto_increment | | long_url | varchar(100) | NO | | NULL | | | short_url | varchar(40) | YES | | NULL | | +-----------+--------------+------+-----+---------+----------------+  分布式和高并发设计 ###注：这部分未实现。我的思路如下：</description>
    </item>
    
    <item>
      <title>Golang服务端技术笔记</title>
      <link>https://www.yeqown.xyz/2018/01/27/Golang%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Sat, 27 Jan 2018 20:58:49 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/01/27/Golang%E6%9C%8D%E5%8A%A1%E7%AB%AF%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/</guid>
      <description>总结使用Golang开发服务端时，使用的基础的工具和部署方式。用于思考不足并优化，提升编码效率。 总体上采用MVCS的软件模式，如下图：
从图中可以看出，MVCS是从MVC进化而来，相比于MVC，增加了Service层。把业务逻辑从Controller层中抽离出来，这样做的好处在于，项目日益庞大之后，将某些功能独立出来。
Golang工具  &amp;ldquo;gvt&amp;rdquo; 依赖管理工具 &amp;ldquo;httprouter&amp;rdquo; 路由及中间件配置 &amp;ldquo;schema&amp;rdquo; 解析请求参数到结构体 &amp;ldquo;beego/validation&amp;rdquo; 结构体校验工具 &amp;ldquo;github.com/go-redis/redis&amp;rdquo; redis操作库 &amp;ldquo;github.com/go-sql-driver/mysql&amp;rdquo; Mysql Driver
 文件结构 --Golang Project |-sh # shell脚本，包括数据库脚本 |-config # 配置文件 |-logs # 日志文件 |-vendor # 项目源码及依赖 | |-github.com # | |-mainfest # gvt 依赖管理文件 | |-app | |-utils | |-controllers | |-models | |-route | |-services |-Dockerfile # docker构建镜像配置文件 |-docker-compose.yml # docker-compose.yml文件 `-entry.go # web服务入口文件 部署方式 采用docker来部署应用。分别编写Dockerfile和docker-composer.yml文件，实例如下：
Dockerfile ##### build####FROMgolang:1.9-alpine AS buildWORKDIR/go/src/web-serverCOPY .</description>
    </item>
    
    <item>
      <title>Drone体验</title>
      <link>https://www.yeqown.xyz/2018/01/27/Drone%E4%BD%93%E9%AA%8C/</link>
      <pubDate>Sat, 27 Jan 2018 00:24:40 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/01/27/Drone%E4%BD%93%E9%AA%8C/</guid>
      <description>相较于Jenkins，Gitlab-CI&amp;hellip;等，尝试Drone的首要原因是，天生的docker支持。不用去操心部署CI或者CD的环境配置等等烦心事。只需要上手，如何配置这个CD工具，让我使用更加畅快和顺手。
安装部署 前提：已经安装了docker，docker-compose，并基本掌握docker用法，基本熟悉docker-compose配置文件
pull镜像 docker pull drone/drone:0.8 # droner-server 镜像 docker pull dorner/agent:0.8 # drone-agent 镜像 也可以跳过这一步，docker运行的时候，如果匹配不到本地镜像，会自动拉取。
docker-compose.yml配置文件 为了方便，新建一个Drone文件夹，目录结构如下：
--Drone # 文件夹 |---docker-compose.yml # docker-compose 配置文件 |---data # 用于挂载的数据文件 |---drone.domain.com # nginx sever 配置文件 `---other.file # 其他文件 已知文件结构后，编写的docker-compose.yml文件如下：
version: &amp;#39;2&amp;#39; services: drone-server: image: drone/drone:0.8 container_name: drone-server ports: - 8000:8000 - 9000:9000 volumes: - ./data:/var/lib/drone/ # 在没有跟数据库绑定的情况下，默认使用sqlite数据库 restart: always environment: - DRONE_OPEN=false - DRONE_HOST=http://127.0.0.1:8000 # 最好是在服务器上，localhost无法收到webhook的通知 - DRONE_ADMIN=yourname - DRONE_GITHUB=true - DRONE_GITHUB_CLIENT=7bc7971bxxxxx # 需要预先注册一个github oauth应用 - DRONE_GITHUB_SECRET=9456c630xxxxxxxxxxxxxx - DRONE_SECRET=secret - DRONE_DEBUG=false drone-agent: image: drone/agent:0.</description>
    </item>
    
    <item>
      <title>docker-compose上手</title>
      <link>https://www.yeqown.xyz/2018/01/24/Docker-Compose%E4%B8%8A%E6%89%8B/</link>
      <pubDate>Wed, 24 Jan 2018 16:01:02 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/01/24/Docker-Compose%E4%B8%8A%E6%89%8B/</guid>
      <description>docker compose 用于快速在集群中部署分布式应用。按我的理解也可以用于简化部署单个应用。譬如我要使用dock er启动一个nginx服务，需要做端口映射，挂载数据文件，指定镜像&amp;hellip;等等，这种情况下，可以将启动容器的命令整合到docker-compose.yml文件中，可以在多个服务器上运行，瞬间就完成了nginx的安装及配置，再也不用去编译，解决环境依赖了，这种感觉实在是太爽了！！！
安装  使用pip pip install docker-compose 从官方Github Release下载二进制包文件 其他方法略去  使用场景  在日常工作中，经常会碰到需要多个容器相互配合来完成某项任务的情况。例如要实现一个 Web 项目，除了 Web 服务容器本身，往往还需要再加上后端的数据库服务容器，甚至还包括负载均衡容器等。
 实战场景 需要部署的项目，只有两个docker容器，一个server，一个db。一般的部署方式是，分别启动两个容器，容器间通过互联的方式通信：
sudo docker run --rm -p 5433:5432 --name postgres -e POSTGRES_PASSWORD=minepwd -e POSTGRES_USER=mineusr -d postgres sudo docker run --rm -p 9091:9091 --link postgres:postgres --name mineserver -d me/mineserver 这两条命令还是有挺麻烦的，如果记不住，当然可以用shell脚本来运行，可以如果其中某一个服务无法如期运行。。。就很监介了。这时候就可以引入docker-compose了。
编写docker-compose.yml来部署项目 version: &amp;#34;2&amp;#34; # 指定docker-compose版本 services: # 项目依赖的服务 postgres: # 服务名字 image: postgres # 服务需要的docker镜像与docker run命令中的镜像指定方式一致 volumes: # 挂载卷，这里的主要目的是，方便同步数据库和数据脚本 - .</description>
    </item>
    
    <item>
      <title>docker&#43;selenium&#43;python构建前端自动化分布式测试环境</title>
      <link>https://www.yeqown.xyz/2018/01/23/docker-selenium-python%E6%9E%84%E5%BB%BA%E5%89%8D%E7%AB%AF%E8%87%AA%E5%8A%A8%E5%8C%96%E5%88%86%E5%B8%83%E5%BC%8F%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83/</link>
      <pubDate>Tue, 23 Jan 2018 11:07:08 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/01/23/docker-selenium-python%E6%9E%84%E5%BB%BA%E5%89%8D%E7%AB%AF%E8%87%AA%E5%8A%A8%E5%8C%96%E5%88%86%E5%B8%83%E5%BC%8F%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83/</guid>
      <description>docker + selenium + python 构建前端自动化分布式测试环境。利用seleninum-grid分布式框架，python编写测试代码，docker部署来进行前端自动化测试
 2018-2-1 更新 使用docker-compose编排
 分布式部署的优点 自动化的优缺点就不再重复了，主要分析下docke部署和分布式的优势
 提高自动化的测试效率（分布式） 方便打包和持续集成（docker） 解决多人coding，却因为路径不一致导致无法运行的问题（当然也可以通过其他方式来解决～）  这里还有一个问题就是：使用docker部署方式运行测试代码，是看不见本地浏览器启动的，因此在调试测试代码的时候，需要一定的工具来协助，譬如VNC viewer
开篇-selenium  大家都知道 Selenium 是支持多种浏览器多个编程语言的一个自动化测试工具。而 Selenium Grid 是一种可以让用户在不同的环境和不同的浏览器上并行运行 web 测试用例的框架。换而言之，使用 Selenium Grid 可以让我们在分布式测试环境下执行测试，例如 Windows，Linux，Mac OS，Andoid/iOS 等等，这样可以大大减少重复的工作量，提高我们的工作效率。
 selenium分布式结构如图： 搭建分布式环境 在Dockerhub已经具有了相应的selenium的镜像，我们直接使用就行了
拉取镜像 docker pull selenium/hub docker pull selenium/node-chorme-debug 关于node-chrome-debug和node-chrome的区别： 暂未研究
运行容器 docker run -d -p 4444:4444 --name sel-hub selunium/hub # 运行hub服务 docker run -d -p 5900:5900 --link sel-hub:hub selunium/node-chrome-debug # 运行slenium chrome 节点 # more node could append like node-chrome-debug 查看节点信息 在浏览器中打开http://127.</description>
    </item>
    
    <item>
      <title>docker探索</title>
      <link>https://www.yeqown.xyz/2018/01/18/docker%E6%8E%A2%E7%B4%A2/</link>
      <pubDate>Thu, 18 Jan 2018 15:02:49 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/01/18/docker%E6%8E%A2%E7%B4%A2/</guid>
      <description>docker 学习实战笔记。关于docker的详细信息参见官方文档
实战环境配置：
   系统 Docker     MacOS 17.12.0-ce    实战一 简单部署可执行文件 本次实战是部署一个golang的web服务, 需要mongo, redis支持
  省略代码，打包过程（Mac用户别忘了打包GOOS=linux，0·0）
  编写Dockerfile
  FROM alpine # 简版linux RUN mkdir /app # 在镜像中创建一个文件夹 COPY ./webapp /app COPY ./config /app/config COPY ./logs /app/logs EXPOSE 35764 # 暴露35764端口，也就是为了从外部可以访问到容器类的服务 WORKDIR /app # 切换到应用文件夹（在镜像执行的时候，会自动切换） 构建镜像 执行命令 docker build -t tp-api:v1 .  执行结果如下：
Sending build context to Docker daemon 55.</description>
    </item>
    
    <item>
      <title>wrk性能测试工具使用总结</title>
      <link>https://www.yeqown.xyz/2018/01/16/wrk%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 16 Jan 2018 11:28:59 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/01/16/wrk%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8%E6%80%BB%E7%BB%93/</guid>
      <description>wrk 压力测试工具的简单小结。 项目地址：https://github.com/wg/wrk
安装 Win: https://github.com/wg/wrk/wiki/Installing-wrk-on-Windows-10 Linux: https://github.com/wg/wrk/wiki/Installing-wrk-on-Linux MacOS: brew install wrk
基本命令 ➜ ~ wrk Usage: wrk &amp;lt;options&amp;gt; &amp;lt;url&amp;gt; Options: -c, --connections &amp;lt;N&amp;gt; 连接数 -d, --duration &amp;lt;T&amp;gt; 持续时间 -t, --threads &amp;lt;N&amp;gt; 线程数 -s, --script &amp;lt;S&amp;gt; 制定lua脚本 -H, --header &amp;lt;H&amp;gt; 添加请求头 --latency 打印延迟分布信息 --timeout &amp;lt;T&amp;gt; 设置请求超时 -v, --version 打印版本信息 &amp;lt;N&amp;gt;表示数字参数，支持国际单位 (1k, 1M, 1G) &amp;lt;T&amp;gt;表示时间参数，支持国际单位 (2s, 2m, 2h) 简单使用及解释 wrk -t1 -d20s -c10 -s post.lua http://api.example.com/fake/post 以单线程 保持10个连接 持续20秒 运行post.lua脚本访问http://api.example.com/fake/post
报告分析 如下是一个简单的性能测试报告</description>
    </item>
    
    <item>
      <title>持续集成-Jenkins</title>
      <link>https://www.yeqown.xyz/2018/01/13/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/</link>
      <pubDate>Sat, 13 Jan 2018 15:23:30 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/01/13/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90/</guid>
      <description>持续集成是一种软件开发实践，即团队开发成员经常集成它们的工作，通过每个成员每天至少集成一次，也就意味着每天可能会发生多次集成。每次集成都通过自动化的构建（包括编译，发布，自动化测试）来验证，从而尽早地发现集成错误。
关于持续集成  要素
 . 统一的代码库 . 自动构建 . 自动测试 . 每个人每天都要向代码库主干提交代码 . 每次代码递交后都会在持续集成服务器上触发一次构建 . 保证快速构建 . 模拟生产环境的自动测试 . 每个人都可以很容易的获取最新可执行的应用程序 . 每个人都清楚正在发生的状况 . 自动化的部署
Jenkins的搭建与使用：  前提：安装好Java环境
 . 下载参见下载地址, 我采用的是java -jar jenkins.war的方式部署 . 可能有用的教程 http://geek.csdn.net/news/detail/95824
更改jenkins服务端口 使用java -jar jenkins.war这样的命令来启动jenkins时会使用默认的端口8080，有些情况下8080端口已经被我们使用了，这个时候如果希望修改这个端口应该怎么办呢？
在命令行后面添加 --httpPort=8899，其实就是配置jetty的启动端口。如下：
set JENKINS_HOME=./ java -Djsse.enableSNIExtension=false -jar path/to/jenkins.war --httpPort=8899 zsh设置Jenkins环境变量 export JENKINS_HOME = &amp;quot;your/jenkins/path&amp;quot; 补充  于2018-1-26日更新
 鉴于尝试了drone和docker, 强烈建议采用docker部署jenkins，或者替换Jenkins为Drone
最后 写得很简略，没有提供配置时候遇到的坑以及解决方法，遇到之后再补上。。。以上hahah</description>
    </item>
    
    <item>
      <title>pytest用法小结</title>
      <link>https://www.yeqown.xyz/2018/01/13/pytest%E7%94%A8%E6%B3%95%E5%B0%8F%E7%BB%93/</link>
      <pubDate>Sat, 13 Jan 2018 15:20:50 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/01/13/pytest%E7%94%A8%E6%B3%95%E5%B0%8F%E7%BB%93/</guid>
      <description>pytest最常用法总结，当然不止这一点功能。关于更多更强大的插件，可以根据自己需要来定制。
 安装  pytest 安装和使用都非常简单, 只需pip install pytest
 编写测试代码  使用pytest，不需要像unittest模块一样，pytest使用的是python自带的assert，如：
def test_global_function(): assert 1 == 1  使用pytest.mark  pytest.mark 用于给测试方法打上标签，在稍后的执行中会讲到如何使用marker
@pytest.mark.marker_self def test_global_function(): assert 1 == 1  使用pytest.fixture  @pytest.fixture def google_url(): return &amp;#34;http://google.com&amp;#34;  setup 和 teardown  setup和teardown方法作用范围，分为全局作用，类作用，方法作用
 全局：作用于全局测试函数 类： 作用于自身类 类方法： 作用于类函数  简单举例: # 全局 def setup_function(function): print(&amp;#34;setup function global&amp;#34;) def teardown_function(function): print(&amp;#34;teardown function global&amp;#34;) # 类 class Test_fixture: @classmethod def setup_class(cls): print(&amp;#34;class setup method&amp;#34;) @classmethod def teardown_class(cls): print(&amp;#34;class teardown method&amp;#34;) # 类方法 def setup_method(self, method): print(&amp;#34;class method setup function&amp;#34;) def teardown_method(self, method): print(&amp;#34;class method teardown function&amp;#34;)  pytest配置文件  配置文件名为pytest.</description>
    </item>
    
    <item>
      <title>git命令清单</title>
      <link>https://www.yeqown.xyz/2018/01/11/git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/</link>
      <pubDate>Thu, 11 Jan 2018 19:23:35 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/01/11/git%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/</guid>
      <description>git命令清单
基本命令 git help &amp;lt;command&amp;gt; # 显示command的help git show # 显示某次提交的内容 git show $id git co -- &amp;lt;file&amp;gt; # 抛弃工作区修改 git co . # 抛弃工作区修改 git add &amp;lt;file&amp;gt; # 将工作文件修改提交到本地暂存区 git add . # 将所有修改过的工作文件提交暂存区 git rm &amp;lt;file&amp;gt; # 从版本库中删除文件 git rm &amp;lt;file&amp;gt; --cached # 从版本库中删除文件，但不删除文件 git reset &amp;lt;file&amp;gt; # 从暂存区恢复到工作文件 git reset -- . # 从暂存区恢复到工作文件 git reset --hard # 恢复最近一次提交过的状态，即放弃上次提交后的所有本次修改 # git ci &amp;lt;file&amp;gt;  git ci . git ci -a # 将git add, git rm和git ci等操作都合并在一起做 git ci -am &amp;#34;some comments&amp;#34; git ci --amend # 修改最后一次提交记录 git revert &amp;lt;$id&amp;gt; # 恢复某次提交的状态，恢复动作本身也创建次提交对象 git revert HEAD # 恢复最后一次提交的状态 查看文件(diff) git diff &amp;lt;file&amp;gt; # 比较当前文件和暂存区文件差异 git diff git diff &amp;lt;id1&amp;gt;&amp;lt;id1&amp;gt;&amp;lt;id2&amp;gt; # 比较两次提交之间的差异 git diff &amp;lt;branch1&amp;gt;.</description>
    </item>
    
    <item>
      <title>数据库索引基础</title>
      <link>https://www.yeqown.xyz/2018/01/11/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Thu, 11 Jan 2018 19:08:44 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/01/11/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B4%A2%E5%BC%95%E5%9F%BA%E7%A1%80/</guid>
      <description>数据库索引的简单介绍和使用注意事项
树 二叉树 性质1：在二叉树中第 i 层的结点数最多为2^(i-1)（i ≥ 1） 性质2：高度为k的二叉树其结点总数最多为2^k－1（ k ≥ 1） 性质3：对任意的非空二叉树 T ，如果叶结点的个数为 n0，而其度为 2 的结点数为 n2，则：n0 = n2 + 1  二叉搜索树 BST 若左子树不空，则左子树上所有结点的值均小于它的根结点的值； 若右子树不空，则右子树上所有结点的值均大于或等于它的根结点的值； 左、右子树也分别为二叉排序树； 没有键值相等的节点  平衡二叉树 AVL树 平衡二叉树（balanced binary tree）,又称 AVL 树。它或者是一棵空树,或者是具有如下性质的二叉树：
它的左子树和右子树都是平衡二叉树， 左子树和右子树的深度之差的绝对值不超过1。  平衡二叉树是对二叉搜索树(又称为二叉排序树)的一种改进。二叉搜索树有一个缺点就是，树的结构是无法预料的，随意性很大，它只与节点的值和插入的顺序有关系，往往得到的是一个不平衡的二叉树。在最坏的情况下，可能得到的是一个单支二叉树，其高度和节点数相同，相当于一个单链表，对其正常的时间复杂度有O(log(n))变成了O(n)，从而丧失了二叉排序树的一些应该有的优点。
B树 BTree是平衡搜索多叉树，设树的度为2d（d&amp;gt;1），高度为h，那么BTree要满足以下条件：
每个叶子结点的高度一样，等于h； 每个非叶子结点由n-1个key和n个指针point组成，其中d&amp;lt;=n&amp;lt;=2d,key和point相互间隔，结点两端一定是key； 叶子结点指针都为null； 非叶子结点的key都是[key,data]二元组，其中key表示作为索引的键，data为键值所在行的数据；  B+树 B+Tree是BTree的一个变种，设d为树的度数，h为树的高度，B+Tree和BTree的不同主要在于：
B+Tree中的非叶子结点不存储数据，只存储键值； B+Tree的叶子结点没有指针，所有键值都会出现在叶子结点上，且key存储的键值对应data数据的物理地址； B+Tree的每个非叶子节点由n个键值key和n个指针point组成；  索引 聚簇索引和非聚簇索引（也叫：聚集和非聚集）  MyISAM 非聚簇索引
 MyISAM存储引擎采用的是非聚簇索引，非聚簇索引的主索引和辅助索引几乎是一样的，只是主索引不允许重复，不允许空值，他们的叶子结点的key都存储指向键值对应的数据的物理地址。非聚簇索引的数据表和索引表是分开存储的。非聚簇索引中的数据是根据数据的插入顺序保存。因此非聚簇索引更适合单个数据的查询。插入顺序不受键值影响。
 InnoDB 聚簇索引
 聚簇索引的主索引的叶子结点存储的是键值对应的数据本身，辅助索引的叶子结点存储的是键值对应的数据的主键键值。因此主键的值长度越小越好，类型越简单越好。聚簇索引的数据和主键索引存储在一起。聚簇索引的数据是根据主键的顺序保存。因此适合按主键索引的区间查找，可以有更少的磁盘I/O，加快查询速度。
但是也是因为这个原因，聚簇索引的插入顺序最好按照主键单调的顺序插入，否则会频繁的引起页分裂，严重影响性能。在InnoDB中，如果只需要查找索引的列，就尽量不要加入其它的列，这样会提高查询效率。
索引类型 主键索引：根据主键pk_clolum（length）建立索引，不允许重复，不允许空值。</description>
    </item>
    
    <item>
      <title>RN历险记</title>
      <link>https://www.yeqown.xyz/2018/01/11/RN%E5%8E%86%E9%99%A9%E8%AE%B0/</link>
      <pubDate>Thu, 11 Jan 2018 18:11:23 +0000</pubDate>
      
      <guid>https://www.yeqown.xyz/2018/01/11/RN%E5%8E%86%E9%99%A9%E8%AE%B0/</guid>
      <description>讲述配置ReactNative的心酸历程
程序猿长征第一步 根据官方文档来安装RN, 以及巨大无比的Xcode Ver9.0.0
错误一：Build Fail 可能描述不太一致, 但是原因都差不多, 文件缺失。 菜鸟想必看到这些个报错, 两眼一懵逼, 啥子情况, 怎么和官方的描述不一致, 一个及其简单的RN-Demo, 我就是想跑一下的喂！
我遇到的情况, 分为两种：
 其一是安装很慢, 之后失败 其二是安装很快, 然后失败
 经过反复的“瞎子”调整, 在多次更换react, react-native版本, 求助Google大叔无果之后。我开始了阅读输出日志的漫漫长路, 终于发现了build失败的元凶, boost/xxx.hpp not found为啥找不到呢, 去文件夹一看, 才发现这些文件真的不存在&amp;hellip;&amp;hellip;
好了知道错误, 就再Google下咯（其实我还去改过这些#includ&amp;lt;boost/config/user.hpp&amp;gt; 0&amp;lt;~&amp;gt;0）这里就直接给出我找的结果吧: http://cdn2.jianshu.io/p/2ef019a7e82a
总的说来, 就是自动下载的的第三方库是残缺的
错误二：CFBundleIdentifier not Found 第二错误也是困扰了比较多人, 我遇到的只是导致这个情况的其中之一
通过查看输出日志, 并没有发现什么有用的信息, 提示的是Command Fail , balabala… 手动搜索了一下PlistBuddy, 了解了下用法, 然后我手动执行了下命令, 居然可以！！！！什么情况, 那为什么提示错误信息？果断进入到文件夹中查看,果然文件是存在的那么为啥一个可以, 一个不可以呢？到这里, 大致猜到原因了, 没有找到文件
再次以此为点求助Google大叔: http://blog.csdn.net/ohyeahhhh/article/details/54691512
这个坑就是, Xcode编译保存的路径和react-native-cli寻找的路径不一致, 通过修改路径就OK啦, 还有其他原因导致的这个fail 请参阅链接, 先搞清楚react-native run-ios做了啥事.
中间更多细节”虐“去~~~~, 感谢预先踩坑的前辈, 最终项目成功运行啦！</description>
    </item>
    
  </channel>
</rss>
